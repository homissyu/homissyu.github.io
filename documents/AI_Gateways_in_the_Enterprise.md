# Chapter 1. AI도입이 기업의 성공에 필수적인 이유

이 장에서는 AI 도입이 엔터프라이즈 성공에 필수적이 된 이유를 살펴보고, 조직들이 비즈니스 기능의 다양한 분야에서 AI를 활용하여 변혁과 경쟁 우위를 이루는 방법을 살펴볼 것입니다.

인공지능은 지난 몇 년간 화제의 중심이 되었습니다. 엔터프라이즈 기술 환경을 근본적으로 변화시키고 있습니다. 과거의 기술 혁명들—문서 작성 및 워크플로우를 획기적으로 변경한 워드프로세서, 조직의 고객 관계 관리 방식을 변혁한 CRM 시스템, 재무 운영을 혁신한 회계 소프트웨어—을 바탕으로 AI는 새로운 차원의 변화를 도입하고 있습니다. 이전의 혁신 기술들이 각각 특정 비즈니스 기능을 변혁했다면, AI의 영향은 매우 광범위하여 고객 경험 향상, 운영 효율화, 그리고 조직 내 거의 모든 부서와 프로세스에서의 혁신을 동시에 추진하고 있습니다.

최근 연구 데이터는 AI 도입의 영향을 보여줍니다. Google Cloud가 실시한 2,500명의 선임 비즈니스 리더를 대상으로 한 포괄적인 글로벌 설문조사에 따르면, 프로덕션 환경에서 AI를 사용하는 74%의 엔터프라이즈가 첫 해 내에 이미 투자 수익률(ROI)을 확인하고 있습니다. 더욱이 AI로부터 수익 성장을 경험하는 조직의 86%는 연간 수익에서 6% 이상의 이득을 보고하고 있습니다.

유사하게, CIO Tech 설문조사에서는 향후 1년 내에 70%의 엔터프라이즈가 향후 3~5년 내에 비즈니스를 변혁할 AI 도구를 사용할 것으로 예상하여 AI 지출을 증가시킬 계획임을 발견했습니다.

이러한 통계는 AI 도입이 단순한 점진적 개선을 이루는 것이 아니라 주요 영역에 걸쳐 비즈니스 변혁을 가능하게 하고 있음을 강조합니다. 아래 통계는 모두 Google Cloud 설문조사로부터 얻었습니다:

    Customer experience
    
    AI는 조직이 고객과 상호작용하는 방식을 향상시킵니다. Google Cloud 설문조사에 따르면 AI를 통해 사용자 경험을 개선하는 85%의 기업이 사용자 참여도의 측정 가능한 증가를 보고하고 있습니다. 챗봇에서부터 맞춤형 추천에 이르기까지 AI는 고객 여정을 재편하고 있습니다.
    
    Productivity
    
    AI로부터 생산성 개선을 보고하는 엔터프라이즈 중 45%는 직원 생산성이 2배 이상 증가했다고 언급합니다. 이는 AI가 반복적인 작업을 자동화하고 업무 수행 방식을 변혁할 수 있는 능력을 강조합니다.
    
    Business growth
    
    AI는 여러 분야에서 성장을 촉진하고 있으며, 63%의 조직이 AI 도입으로 인한 직접적인 비즈니스 개선을 보고하고 있습니다:
    
    - 45%의 조직은 AI 배포 후 엔지니어링 및 개발자 생산성이 최소 2배 이상 증가했다고 봅니다.
    
    - 84%의 조직은 기술 팀의 가속화와 6개월 이내에 AI 사용 사례를 프로덕션으로 이동할 수 있게 되었다고 보고합니다.
    
    - 70%의 조직은 더 빠른 인사이트 도출 시간을 보고하며, 59%는 핵심 IT 프로세스 및 인프라의 정확도를 개선했습니다.
    
    Security
    
    AI를 통해 보안 태세를 개선한 조직의 56% 중 82%는 더 나은 위협 탐지를 언급하며, 71%는 사건 대응 시간 단축을 보고합니다.

개념 증명에서 프로덕션 시스템으로의 전환은 AI 도입의 분수령을 표시합니다. 오늘날 84%의 조직은 아이디어에서 프로덕션까지 6개월 이내에 AI 사용 사례를 배포할 수 있으며, 이는 이전 기술 혁신과 비교했을 때 놀라운 가속화입니다. 이전 기술들이 특정 비즈니스 기능을 변혁했던 것과 달리, 이러한 빠른 배포 능력은 AI가 인프라와 핵심 프로세스에서부터 고객 대면 서비스와 비즈니스 전략에 이르기까지 조직의 전체 운영 환경에 걸쳐 동시에 변화를 이끌 수 있게 합니다. 이러한 다중 영역에 걸친 수평적 변혁은 기술이 엔터프라이즈를 재편하는 방식의 근본적인 변화를 나타냅니다.

AI 도입은 더 이상 경쟁 관련성을 유지하려는 조직에게 선택 사항이 아닙니다. 초기 AI 도입 기업들은 AI 솔루션 구현에 대한 실무 경험을 축적하고, 팀 전체에 걸쳐 깊은 AI 전문성을 개발하고, AI 통합을 최적화하도록 워크플로우를 재편성하고, AI 모델을 지속적으로 개선하는 평가 피드백 시스템(평가)을 만들어 경쟁 우위를 창출하고 있습니다.

규모에 맞는 AI 구현은 아키텍트와 플랫폼 엔지니어에게 상당한 기술적 과제를 제시합니다. 개념 증명에서 프로덕션으로 이동하는 조직들은 보안, 거버넌스, 확장성 관련 복잡한 요구사항에 직면합니다. AI를 성공적으로 배포하려면 다음을 수행할 수 있는 견고한 기술적 기초를 구축해야 합니다:

- 여러 AI 공급자에 걸쳐 민감한 데이터 및 API 자격증명 보안

- 규모에 맞는 리소스 소비 관리 및 최적화

- 일관된 성능 및 안정성 보장

- 조직 전체의 규정 준수 및 거버넌스 유지

- 기존 시스템 및 워크플로우와의 원활한 통합 실현

이러한 기술적 과제를 성공적으로 해결하면서 빠른 배포와 장기 전략의 균형을 맞추는 조직들은 AI 기반 세계에서의 성공을 위한 위치를 확보하게 될 것입니다.

## Common AI Use Cases
빅데이터 아키텍처의 등장으로 촉진되어 AI 기술은 마침내 학술 연구 센터에서 주류로 이동했습니다. 대규모 언어 모델(LLM)과 같은 새로운 도구의 부상은 팀이 협업하는 방식부터 기업이 데이터를 분석하는 방식까지 모든 것을 변혁하고 있습니다.

    Note
    
    LM은 광대한 양의 텍스트 데이터로 훈련된 인공지능 시스템으로, 인간과 같은 텍스트를 이해하고 생성합니다. LLM의 '대규모(Large)'는 훈련 데이터의 크기와 모델의 매개변수(그 동작을 결정하는 수학적 값)의 크기를 모두 나타냅니다. LLM은 훈련 데이터와 문맥을 바탕으로 수열에서 가장 가능성 있는 다음 단어를 예측하여 작동합니다. 문맥을 이해하고 질문에 답하고 텍스트를 작성할 수 있는 자동완성 시스템이라고 생각하면 됩니다. LLM의 주요 사례로는 GPT-4, Claude, Llama가 있습니다.

오늘날 기업들이 AI를 활용하는 주요 방식들을 살펴보겠습니다. 새로운 사용 사례가 정기적으로 등장한다는 점을 유념하면서 말입니다. 성공의 핵심은 AI가 귀사에 가장 큰 가치를 제공할 수 있는 분야를 파악하고, 인간의 능력을 대체하기보다는 향상시키는 방식으로 이를 구현하는 것입니다.

### Enterprise Productivity and Knowledge Work

AI는 데이터로부터 힘을 얻습니다. 데이터는 AI에서 이중의 역할을 수행합니다: LLM이 훈련되는 기초를 형성하고 그들의 작동을 위한 중요한 문맥으로 기능합니다. 금융 분석가들은 AI를 사용하여 분기별 보고서와 시장 동향에서 통찰력을 빠르게 추출하여, 수시간의 수작업 분석을 수분의 자동화된 처리로 변환합니다. 마케팅 팀은 AI를 활용하여 캠페인 성과 데이터를 분석하고 콘텐츠 변형을 생성합니다. 연구팀은 AI를 사용하여 학술 논문을 요약하고 방대한 발행물 데이터베이스에서 관련 연구를 식별합니다.

LLM은 문서 분석 및 요약, 보고서 생성 및 데이터 시각화, 시장 조사 및 동향 분석, 이메일 관리 및 응답 초안 작성, 회의 요약 및 조치 항목 추출 등과 같은 작업에 강력합니다.

### Customer Experience and Engagement

고객 서비스는 조직들이 AI 구현에서 즉각적인 ROI를 보는 최상위 영역 중 하나입니다.

현대의 AI 기반 고객 서비스 시스템은 단순히 기본 질문에 답하는 것 이상을 수행합니다. 고객이 타이핑하든 말하든 간에 자연어를 이해하고 처리하여 의도를 파악합니다. 대화 흐름은 모든 과거 상호작용, 구매 이력, 계정 세부 정보로 풍부하고 개인화됩니다. 복잡한 문제가 발생하면 이러한 시스템은 고객을 전문가에게 지능적으로 라우팅하는 방법을 알고 있으며 AI가 생성한 문맥, 실시간 통찰력, 응답 제안을 제공합니다.

### Business Process Automation and Decision Support

AI가 핵심 비즈니스 프로세스에 미치는 영향은 이 기술이 단순한 스크립트된 자동화를 어떻게 초월하는지 보여줍니다. 현대 AI는 의사결정에 있어 전략적 파트너로 기능합니다. 단순히 라우팅 작업을 처리하는 것이 아니라 팀이 더 나은 의사결정을 내리도록 돕습니다. 방대한 양의 데이터를 즉시 처리하고 패턴을 감지하며 최적화를 제안할 수 있는 조수라고 할 수 있습니다.

비즈니스 프로세스 자동화 및 의사결정 지원에서 AI의 일반적인 응용 분야는 다음과 같습니다:

- 자동화된 문서 처리 및 데이터 추출
- 지능형 워크플로우 라우팅
- 사기 탐지 및 위험 평가
- 공급망 최적화
- 수요 예측
- 자원 할당
- 품질 관리 및 이상 탐지
- 예측 유지보수

이러한 시스템은 방대한 양의 데이터를 처리해 패턴을 식별하고, 예측을 수행하며, 사람이 수작업으로는 발견할 수 없는 최적화 방안을 제안할 수 있습니다.

### Software Development and Engineering

소프트웨어 개발 및 엔지니어링 분야에서 AI는 인간의 창의성과 전문성을 증강시키고, 반복적인 작업을 줄여 개발자 경험을 개선함으로써 개발자가 비즈니스 로직에 더 많은 시간을 할애할 수 있게 합니다. 그러나 아직 AI가 인간을 능가하거나 완전히 대체한 것은 아니며, 인간의 감독 필요성은 사라지지 않았습니다. 개발팀은 이제 코드 개선을 제안하고, 테스트 과정을 자동화하며, 시스템 상태 유지를 돕는 AI 도구와 함께 작업하고 있습니다. AI 코딩 어시스턴트와 함께 일하는 경험은 놀라울 정도입니다. 마치 24시간 내내 옆에서 지켜보는 또 다른 개발자가 있는 것 같은데, 그 개발자는 즉시 코드 패턴과 모범 사례를 떠올릴 수 있습니다.

GitHub의 Copilot, Vercel의 v0, StackBlitz의 Bolt.new, 그리고 OpenAI나 Anthropic의 기본 모델과 같은 AI 기반 코드 생성 도구 및 어시스턴트는 개발자에게 매우 귀중하며 다음을 수행할 수 있습니다:

- 코드 스니펫 및 완전한 함수 생성

- 최적화를 제안하고 잠재적 버그 식별

- 테스트 프로세스 자동화

- 문서 유지보수 지원

- 코드 리뷰 보조

- API 문서 생성

- 데이터베이스 쿼리 생성 및 최적화

- 복잡한 문제 디버깅

### Sales and Marketing

조직들은 정교한 고객 세분화 및 타겟팅을 위해 AI를 활용하고 있으며, 콘텐츠 개인화를 통해 대규모로 맞춤형 경험을 제공하고 있습니다.

AI 시스템은 리드 스코어링 및 적격성 판단에 탁월하여 영업팀이 가장 유망한 기회에 노력을 집중하도록 도와줍니다. 마케팅팀은 AI 기반 캠페인 최적화와 포괄적인 소셜 미디어 모니터링 및 분석에서 혜택을 받으며, AI를 활용해 마케팅 카피를 생성하고 A/B 테스트를 더 효과적으로 분석합니다.

이 기술은 예측 분석을 통해 고객 이탈을 방지하고, 지능형 고객 행동 분석을 통해 전환율을 최적화합니다. 종합적으로, 이러한 AI 기반 도구들은 팀이 더욱 효과적인 캠페인을 만들고, 유망한 리드를 발굴하며, 전례 없는 정확도로 마케팅 지출을 최적화할 수 있게 해줍니다.

### Health Care

의료 제공자들은 정교한 의료 이미지 분석, 더욱 정확한 질병 진단, 그리고 맞춤형 치료 계획을 위해 AI를 활용하고 있습니다.

이 기술은 지속적인 환자 모니터링과 자동화된 약물 상호작용 확인을 가능하게 하며, 의료 기록 관리 및 예약 일정 관리를 간소화합니다. AI는 또한 환자와 적절한 임상 시험의 매칭을 촉진하고 포괄적인 인구 건강 분석을 가능하게 합니다.

AI를 통해 의료 제공자들은 비용과 행정 부담을 동시에 줄이면서 더욱 나은 환자 돌봄을 제공할 수 있으며, 궁극적으로는 더욱 효율적이고 효과적인 의료 시스템으로 이어집니다.

## The Challenges of Implementing AI and LLMs
조직들이 실험 단계의 AI 프로젝트에서 프로덕션 배포로 이동할 때, 전통적인 네트워킹 인프라가 대처하도록 설계되지 않은 복잡한 과제에 직면하게 됩니다. 효과적이고 안전하며 통제 가능한 AI 시스템을 구축하기 위해서는 이러한 과제들을 이해하는 것이 매우 중요합니다.

### Security and Access Control
조직들이 AI를 도입할 때 직면하는 가장 심각한 보안 과제는 데이터 개인정보 보호 및 제공자 신뢰를 중심으로 합니다. 조직들은 자신의 독점 정보나 민감한 정보가 훈련 데이터 세트에 포함되거나 제공자 침해를 통해 노출될 위험을 고려해야 합니다. 손상된 제공자 시스템이 원본 데이터뿐만 아니라 조직의 통찰력이나 전략을 드러낼 수 있는 쿼리 및 응답도 노출시킬 수 있다는 점을 감안할 때, 이러한 우려는 특히 심각합니다.

이러한 근본적인 우려 외에도, 조직들은 AI 제공자에 대한 접근을 안전하고 효율적으로 관리하는 실질적인 과제에도 직면해야 합니다. 전통적인 API와 달리, AI 서비스는 다양한 사용 패턴과 보안 요구사항에 적응할 수 있는 동적 접근 제어가 필요합니다. 라우터 및 방화벽과 같은 전통적인 네트워킹 도구는 자격증명 관리나 전송 중인 데이터의 의미론적 이해를 위한 프로비저닝이 부족하기 때문에 충분하지 않습니다.

조직들은 API 키 확산 문제로 자주 어려움을 겪고 있으며, 여기서 키가 실수로 애플리케이션에 하드코딩되거나 구성 파일에 노출될 수 있습니다. 이 문제는 팀 및 애플리케이션 전반에 걸친 일관되지 않은 접근 제어로 인해 더욱 복잡해져서 보안 표준을 유지하기 어렵게 만듭니다. 추가적으로, 조직들은 개인식별정보(PII), 건강보험이동성과책임법(HIPAA), 그리고 일반 데이터 보호 규정(GDPR)에 대한 복잡한 규제 데이터 준수 요구사항을 탐색해야 하며, 전통적인 보안 도구들은 의미론적 이해의 부족으로 인해 이를 처리할 수 없습니다.

### Prompt Management and Data Protection
직원들이 LLM과 상호작용할 때, 프롬프트의 맥락으로 민감한 회사 또는 고객 정보를 포함시킬 수 있습니다. 마찬가지로 LLM도 의도하지 않게 응답에 민감한 정보를 포함시킬 수 있습니다. 이는 생산적인 AI 사용을 허용하면서도 허가되지 않은 데이터 노출을 방지하는 방법이라는 중대한 보안 과제를 야기합니다.

조직들은 민감한 정보를 탐지하고 마스킹하기 위한 강력한 가드레일을 포함하는 포괄적인 프롬프트 관리 기능이 필요합니다. 이러한 시스템은 모든 상호작용 간에 일관된 프롬프트 확강을 제공하면서 프롬프트 구조 및 콘텐츠에 대한 표준화된 정책을 유지해야 합니다. 웹 애플리케이션 방화벽(WAF)이 전통적인 웹 애플리케이션을 보호하는 방식과 유사하게, AI 시스템은 데이터 보안 및 규정 준수를 보장하기 위해 특화된 입력 검증 및 출력 제약이 필요합니다.

### Consumption Control and Resource Optimization
OpenAI, Anthropic 등의 LLM 제공자들은 REST API를 통해 모델을 제공합니다. 기술적 수준에서 LLM 사용은 다른 API 호출과 유사합니다. 텍스트가 포함된 요청을 보내고 모델의 응답을 받습니다. LLM의 이러한 소비 기반의 특성은 비용 관리 및 리소스 최적화를 위한 고유한 과제를 야기합니다. 조직들은 특히 사용 패턴이 크게 다양할 때 여러 팀과 프로젝트 전반에 걸쳐 비용을 할당하는 데 어려움을 겪습니다. 이는 토큰 가중 제어를 통한 리소스 소비 추적 및 제어를 위한 정교한 시스템이 필요합니다.

의미론적 캐싱은 최적화를 위한 특별한 기회를 제시합니다. 많은 유사한 쿼리가 중복 API 호출을 하지 않고 캐시에서 제공될 수 있기 때문입니다. 조직들은 또한 품질 표준을 유지하면서 각 사용 사례에 가장 비용 효율적인 모델을 선택하는 지능형 모델 선택 전략을 구현해야 합니다.

### Resilience and Performance
전통적인 로드 밸런싱 전략은 AI 시스템에서 부족한데, 쿼리의 의미론적 특성이나 지연시간(첫 토큰까지의 시간, TTFT) 및 처리량(초당 토큰 수, TPS)과 같은 주요 성능 지표를 고려하지 않기 때문입니다. 조직들은 단순한 요청 패턴이 아닌 의미론적 이해를 기반으로 유사한 쿼리를 효과적으로 캐시하고 워크로드를 분산시킬 수 있는 지능적이고 상황인식적인 시스템이 필요합니다.

AI 시스템의 페일오버 메커니즘은 전통적인 애플리케이션보다 더 정교해야 합니다. 조직들은 일관된 TTFT와 TPS를 유지하면서 성능, 비용 또는 특정 사용 사례 요구사항에 따라 다양한 AI 제공자 또는 모델 간에 전환해야 하기 때문입니다. 비용 관리는 또 다른 복잡성 계층을 추가합니다. LLM은 종량제 방식으로 작동하며, 비용은 LLM이 처리하는 텍스트의 개별 단위인 토큰을 기반으로 계산되기 때문입니다. 예를 들어 "dog"는 1개의 토큰으로 계산되지만, "companionship"과 같은 긴 단어는 여러 토큰으로 분해될 수 있습니다. 이 토큰 기반 가격 책정 모델은 페일오버 결정이 성능 요구사항과 비용 영향을 균형있게 고려해야 함을 의미합니다.

이러한 과제들은 단순한 애플리케이션 수준의 전처리 또는 기본 라우팅 로직으로는 효과적으로 해결할 수 없습니다. 이러한 접근 방식을 사용하려면 각 애플리케이션이 독립적으로 복잡한 의미론적 분석을 구현하고, 여러 공급업체 연결을 유지하며, 페일오버 로직을 처리하고, 토큰 기반 비용 최적화를 관리해야 하므로, 상당한 작업의 중복을 야기하고 조직 전체에서 불일치의 위험을 증가시킵니다. 또한 애플리케이션 수준의 솔루션은 효과적인 비용 관리 및 규정 준수 모니터링에 필요한 중앙 집중식 가시성을 부족하게 됩니다.

여기서 AI 게이트웨이가 나옵니다. 이들은 애플리케이션과 AI 서비스 사이에 위치하여 이러한 과제들에 대한 포괄적인 솔루션을 제공하는 특화된 도구입니다. AI 게이트웨이는 AI 운영을 관리하기 위한 제어 평면 역할을 하며, 조직 전체의 의미론적 요청 라우팅, 지능형 캐싱, 비용 최적화, 그리고 AI 제공자 페일오버 기능을 제공합니다. 이러한 문제들을 애플리케이션 수준이 아닌 인프라 수준에서 처리함으로써, AI 게이트웨이는 조직이 모든 AI 워크로드에 걸쳐 일관된 정책과 최적화를 구현할 수 있게 하면서 애플리케이션 개발자의 복잡성을 줄입니다.조직들이 AI의 변혁적 잠재력을 이해하고 있음에도 불구하고, 기존 엔터프라이즈 환경으로의 성공적인 통합을 방해할 수 있는 몇 가지 근본적인 장애물이 있습니다.

대부분의 기업들은 AI를 염두에 두고 설계되지 않은 레거시 시스템의 복잡한 환경에서 운영됩니다. 이러한 시스템들은 종종 구식 데이터 형식, 호환되지 않는 API, 그리고 경직된 아키텍처를 사용하며, 데이터는 고립된 관계형 데이터베이스와 SharePoint 같은 클라우드 기반 문서 저장소에서부터 파일 서버와 직원 워크스테이션에 흩어져 있는 플랫 파일에 이르기까지 다양한 소스에 사일로화되어 있습니다. 일부 정보는 종이 기록이나 레거시 오프라인 미디어와 같은 물리적 형식으로만 존재할 수도 있습니다. 이러한 단편화된 데이터 환경과 경직된 아키텍처가 결합되면서 AI 통합을 특히 어렵게 만듭니다. 많은 AI 애플리케이션에 필수적인 실시간 데이터 접근은 이러한 분산되고 이질적인 레거시 인프라를 다룰 때 특히 문제가 됩니다.

기술적 과제 외에도, 조직들은 상당한 운영상 장애물에 직면합니다. AI 기반 시스템으로의 전환은 새로운 기술, 역할, 그리고 프로세스를 필요로 합니다. 팀들은 프롬프트 엔지니어링, 모델 선택, 그리고 AI 운영에 대한 전문성을 개발해야 합니다. AI 도입에 대한 저항을 극복하려면 종종 상당한 훈련 투자와 문화 변화가 필요합니다.

AI 시스템은 입력 데이터만큼만 좋습니다. 조직들은 일관되지 않은 데이터 형식, 시스템 전반의 구식 정보, 그리고 실시간 동기화 과제로 인해 어려움을 겪습니다. 적절한 데이터 표준화 및 정제 프로세스가 없으면, AI 구현은 신뢰할 수 없거나 일관되지 않은 결과를 제공할 수 있습니다.

엔터프라이즈 전체에 AI 기능을 통합하려면 신중한 아키텍처 고려가 필요합니다. 조직들은 여러 AI 제공자를 관리하고, 다양한 응답 형식을 처리하며, 강력한 오류 처리를 구현하고, 시스템 안정성을 보장해야 합니다. 이러한 복잡성은 새로운 AI 통합 지점이 추가될 때마다 증가하므로, 중앙 집중식 관리 및 모니터링이 필수적입니다.

이러한 장애물들은 엔터프라이즈 AI 배포를 위한 필요한 제어 및 최적화를 제공하면서 복잡성을 추상화할 수 있는 솔루션의 필요성을 강조합니다.

## Overview of the AI Gateway Solution
AI 게이트웨이는 요청과 응답을 의미론적으로 이해하여 AI 상호작용을 처리하고 관리할 수 있는 특화된 API 게이트웨이입니다. 이러한 이해 능력은 기존 네트워킹 솔루션이 제공할 수 없는 기능을 활성화합니다. AI 트래픽을 위한 지능형 중개자로서 역할하며, 애플리케이션과 AI 서비스의 상호작용을 강화하는 정교한 관리 기능을 제공합니다. 이러한 지능은 여러 가지 이유로 매우 중요합니다.

첫째, 게이트웨이는 방화벽과 같은 기존 네트워킹 구성요소가 제공할 수 없는 정교한 자격증명 및 데이터 관리를 제공합니다. API 키를 안전하게 처리하고, 민감한 데이터 노출을 방지하며, 규제 요구사항 준수를 보장합니다. 이러한 과제들은 조직이 AI 사용을 확대함에 따라 점점 더 복잡해집니다. 더욱이, AI 게이트웨이는 여러 LLM 인스턴스를 포함한 모든 인프라 전체에서 이러한 우려사항들의 처리를 일관되게 만들어줍니다.

둘째, AI 특화 성능 최적화를 구현합니다. 게이트웨이는 의미론적 콘텐츠를 기반으로 요청을 지능형으로 로드 밸런싱할 수 있으며, 유사한 쿼리를 캐싱하여 비용과 지연시간을 줄이고, 필요할 때 서로 다른 AI 제공자 간의 정교한 페일오버를 관리할 수 있습니다.

셋째, LLM 운영에 대한 포괄적인 가시성과 제어를 제공합니다. 조직은 사용 패턴을 모니터링하고, 토큰 소비를 기반으로 비용 제어를 구현하며, 해로운 또는 규정 미준수 LLM 상호작용을 방지하는 가드레일을 강제할 수 있습니다. 이러한 기능들은 조직이 초기 AI 실험에서 프로덕션 배포로 확대할 때 필수적입니다.

다음 장에서 살펴볼 것처럼, 기존 네트워킹 접근 방식은 AI 시스템의 이러한 고유한 측면들을 처리하지 못합니다. 이러한 한계와 AI 게이트웨이가 어떻게 이를 해결하는지를 이해하는 것은 견고하고 안전하며 확장 가능한 AI 인프라를 구축하려는 조직에게 매우 중요합니다. 보안 및 규정 준수, 복원력, 운영 제어 관련 구체적인 과제들을 살펴보고, AI 게이트웨이 구현을 위한 아키텍처 패턴과 모범 사례를 심화 학습할 것입니다.

# Chapter 2. The AI Gateway: Bridging the Gaps

조직들이 AI와 LLM을 채택함에 따라 아키텍트와 IT 리더는 이러한 변혁적 기술의 안전하고 안보된 도입을 우선시해야 합니다. 이 장에서는 애플리케이션이 LLM과 통합할 때 발생하는 새로운 과제와 기존 접근 방식—예컨대 기존 네트워킹—이 이 새로운 패러다임에 맞게 설계된 특화된 도구를 포함하도록 어떻게 발전해야 하는지 살펴보겠습니다. 스마트 중개자인 AI 게이트웨이 사용을 살펴보고, 이것이 AI/LLM 상호작용의 보안, 관찰성, 성능을 어떻게 개선하는지 그리고 이것이 기존 네트워킹을 어떻게 보완하는지 알아보겠습니다.

기존 네트워킹이 AI 시스템에 대처하지 못하는 이유

기존 네트워킹이 어디에서 부족한지 이해하기 위해 일반적인 LLM 상호작용을 살펴보겠습니다. LLM은 일반적으로 API 인터페이스를 통해 노출됩니다. 많은 LLM이 OpenAI REST API를 표준화했지만 다른 옵션도 존재합니다(예: Amazon Bedrock 및 Google Vertex). LLM을 호출하려면 클라이언트가 헤더에 적절한 API 키를 포함하고 LLM을 위한 적절한 프롬프트 메시지를 포함한 HTTP 요청을 준비하여 네트워크를 통해 보냅니다. 예를 들어, 명령줄에서 OpenAI를 호출하는 클라이언트는 다음과 같은 요청을 준비할 것입니다:

curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
일반적인 엔터프라이즈 시스템에서, 이 HTTP 호출은 그림 2-1에 표시된 대로 네트워크를 통해 흐릅니다. HTTP 요청을 구성하는 패킷들은 라우터를 통해 흐르다가 "이 패킷이 특정 IP 주소나 네트워크로 이동할 수 있는가?"와 같은 정책을 시행하는 책임이 있는 방화벽에 도달합니다. 그곳에서 L3/L4 인터넷 게이트웨이를 통해 인터넷으로 라우팅됩니다. OpenAI 서비스는 요청을 수신하고 처리하여 결과를 반환하고, 이 결과는 엔터프라이즈 네트워크를 통해 다시 흐르다가 결국 클라이언트로 돌아갑니다.

그림 2-1. 방화벽과 라우터를 포함하는 일반적인 엔터프라이즈 네트워크를 통해 클라이언트에서 LLM으로 요청이 흐르는 방식

"이것이 단순한 HTTP 요청이라면 LLM의 뭐가 특별한가?"라고 궁금해할 수도 있습니다. 좋은 질문입니다. 이 단순한 LLM API 호출은 기존의 HTTP 요청과 유사한 많은 시나리오들을 제시하지만, 특화된 API 도구를 필요로 하는 문제를 야기할 수 있는 몇 가지 차이점도 도입합니다. 이러한 문제들이 무엇인지 살펴보겠습니다.

Security Practices
장 먼저 눈에 띄는 것은 LLM에 대한 호출이 헤더 중 하나로 전송되는 API 키를 필요로 한다는 점입니다. 이 API 키는 민감한 정보이며 전송 중에 암호화되도록 HTTPS 요청에서만 전송되어야 합니다. OpenAI API를 호출할 때(즉, https://api.openai.com을 호출함으로써) 이는 투명하게 발생합니다. 하지만 요청에 사용될 키를 저장하고 검색하는 것은 어떻게 해야 할까요? 개발자들은 어떻게 키를 안전하게 유지할까요? 키가 악의적인 행위이든 다른 팀과 공유함으로써이든 손상되면 보안 문제가 됩니다.

안타깝게도, 규모가 커질수록 이는 프로덕션 환경에 산재된 많은 민감한 API 키들과 이 키들이 손상될 가능성이 높아진다는 것을 의미합니다. 모든 팀이 항상 올바른 보안 관행을 따르지는 않습니다. 예를 들어, 개발자들이 실수로 API 키를 코드에 하드코딩하고 그 코드를 저장소에 커밋하는 것이 흔한 시나리오입니다. 또한, 불행하게도 흔한 관행은 API 키를 환경별 구성 파일에 명시하는 것입니다. 이러한 키 누출은 매우 비용이 많이 들 수 있습니다.

AI 게이트웨이는 기존 네트워킹 보안을 보완하여 이러한 과제들을 해결할 수 있습니다. 이를 어떻게 수행하는지는 제3장에서 다룰 것입니다.

Resilience
지연시간과 로드 밸런싱은 LLM을 엔터프라이즈 워크플로우에 통합할 때 상당한 도전 과제를 제시합니다. 라운드-로빈이나 연결 기반 전략과 같은 전통적인 로드 밸런싱 방법은 너무 단순해서 프롬프트의 의미론적 의미를 고려하지 않거나 어떤 모델이 요청을 처리하기에 가장 적합한지 결정하는 데 도움이 되지 않기 때문입니다. 또한, 의미론적 이해가 없으면 반복되는 쿼리에 대한 응답을 캐시하는 효과적인 방법이 없어서 지연시간과 서버 부하를 크게 줄일 수 있습니다. 엔터프라이즈는 유사하거나 동일한 의미론적 쿼리를 캐시하고 워크로드를 더 효과적으로 분산시켜 응답 시간을 최소화하고 시스템 성능을 개선할 수 있는 지능적이고 상황인식적인 시스템이 필요합니다.

복잡성을 더하는 것은 엔터프라이즈가 종종 전략적 동기, 비용 또는 특화된 사용 사례를 기반으로 OpenAI, Anthropic 또는 맞춤형 도메인 특화 모델과 같은 모델 제공자 간에 전환할 수 있는 유연성이 필요하다는 점입니다. 이는 성능을 유지하면서 워크로드를 동적으로 재라우팅하고, 다운타임을 최소화하며, 범용 모델이든 특정 요구사항에 맞게 설계된 모델이든 모델 간에 원활한 전환을 보장할 수 있는 인프라가 필요합니다.

또 다른 우려사항은 모델 페일오버와 안정성입니다. AI 모델은 피크 사용 시간 동안의 높은 지연시간이나 정확한 응답을 제공하지 못하는 오래된 훈련 데이터와 같은 모델 특화 문제로 인해 성능 저하를 경험할 수 있습니다. 이러한 모델에 대한 견고한 페일오버 메커니즘을 보장하는 것이 필수적입니다. 예를 들어, 엔터프라이즈는 주 모델이 적절하게 수행하지 못할 때 백업 모델이나 대체 제공자로 트래픽을 원활하게 리다이렉트하는 전략이 필요합니다. 이러한 유형의 페일오버는 모델 출력 품질과 비용의 차이를 고려하면서 일관된 사용자 경험을 유지해야 하기 때문에 표준 시스템보다 더 복잡합니다.

Cost Control and Observability
비용 제어는 LLM을 워크플로우에 통합하는 조직에게 중요한 고려사항입니다. LLM 사용의 동적 특성은 누가 이러한 서비스를 호출하는지, 어떤 예산으로, 얼마만큼 지출하는지 추적하기 어렵게 만듭니다. 적절한 가시성이 없으면, 엔터프라이즈는 팀과 프로젝트 전반에 걸쳐 예측 불가능하게 사용이 증가함에 따라 비용이 급증할 위험에 빠집니다. LLM 사용을 특정 팀이나 애플리케이션에 할당하고 모니터링하는 메커니즘을 구현하면 명확성을 제공할 수 있습니다. 더욱이, 토큰 소비(토큰은 LLM 특화 개념입니다)를 기반으로 또는 엔터프라이즈에서 정한 할당량을 기반으로 요청 속도 제한을 설정하면 리소스가 효율적으로 그리고 예산 내에서 사용되도록 보장합니다. 이러한 제한은 또한 거부 서비스(DoS) 조건이나 중요한 운영을 위한 리소스 고갈로 이어질 수 있는 과도한 사용을 방지하는 데 필수적입니다.

관찰성은 LLM 상호작용을 관리하는 또 다른 중요한 측면입니다. 엔터프라이즈는 LLM 워크플로우에서 성능 병목 현상이나 시스템 문제를 식별하기 위해 오류율 및 지연시간과 같은 주요 네트워킹 메트릭을 모니터링해야 합니다. 그러나 LLM은 전통적인 관찰성 관행을 넘어서는 추가적이고 모델 특화된 우려사항들을 도입합니다. 예를 들어, 프롬프트 진화 추적(프롬프트가 과도하게 길지 않은가? 구조가 최적이 아닌 결과를 제공하도록 변경되고 있지 않은가?), 토큰 지연시간(TTFT, 출력 토큰당 시간 등), 문맥 윈도우 사용량, 그리고 사용량 할당은 모두 우려사항입니다.애플리케이션 개발자들은 자신의 애플리케이션의 일부를 구현하기 위해 자유롭게 LLM을 호출할 수 있지만, 프롬프트에서 전송되는 정보의 민감성을 얼마나 자주 고려할까요? 엔터프라이즈 환경에서 민감한 데이터를 전송하는 것은 단순한 간과가 아니라 주요 규정 준수 위험입니다. 엔터프라이즈는 엄격한 데이터 보호 및 비밀유지 요구사항 하에서 운영됩니다. 이러한 제약을 준수하지 않으면서 데이터를 이리저리 보낼 수는 없습니다. LLM으로 작업할 때도 마찬가지입니다. PII이든, HIPAA에 따른 건강 관련이든, GDPR과 같은 지역 보호 대상이든 민감한 데이터는 규제 요구사항을 충족시키기 위해 최고 수준의 주의로 처리되어야 합니다. 실제로, 규정 준수 감시 대상이 아닐 수도 있지만 극도로 민감한 가장 중요한 데이터는 회사의 독점 데이터와 영업 비밀입니다. 모든 팀이 PII와 같은 규제 데이터를 다루어야 하는 것은 아니지만, 대부분의 팀은 민감한 회사별 데이터를 다룹니다.

여기서 가드레일이 등장합니다. LLM 쿼리를 위한 WAF로 생각하면 됩니다—무엇이 수용 가능한지에 대한 규칙을 적용하고, 입력을 검증하며, 출력을 제약합니다. 이러한 가드레일은 AI 운영을 확대함에 따라 중요합니다. 예를 들어, 편향되거나 해로운 응답을 유도하거나 민감한 정보 노출 위험이 있는 프롬프트를 차단할 수 있습니다. 이제 개발자들이 이러한 보호 기능을 앱 코드에 베이킹할 수 없을까라고 궁금해할 수도 있습니다. 물론 할 수 있지만 그러한 접근 방식은 위험합니다. 우리가 사이버 보안의 전체 부담을 개발자에게 맡기지 않듯이, 같은 원칙이 여기에도 적용됩니다. "심층 방어(Defense-in-depth)" 전략이 필수적입니다. 엔터프라이즈는 애플리케이션 수준의 보호 위에 견고하고 외부화된 제어를 계층화하여 사이버 보안에 성공하고, LLM에도 같은 접근 방식이 적용됩니다. 비용 제어, 관찰성, 지능형 가드레일을 결합하는 잘 설계된 거버넌스 계층은 단순한 추가 사항이 아니라—책임감 있게, 예측 가능하게, 그리고 자신감 있게 AI를 확대하기 위한 핵심입니다.

복원력, 가드레일, 보안과 같은 LLM 특화 과제들을 해결하려면 이러한 과제들을 고려할 수 있는 특화된 도구가 필요합니다. 기존 네트워킹은 AI 게이트웨이와 같은 특화된 도구로 보강되어 이러한 과제들을 해결할 수 있습니다. 다음 섹션에서는 AI 게이트웨이에 접근하는 방법을 살펴봅니다.

AI 게이트웨이는 요청의 세부사항을 의미론적으로 이해하여 보안, 가드레일, 관찰성, 페일오버, 로드 밸런싱을 더 잘 구현할 수 있는 특화된 API 게이트웨이입니다. AI 게이트웨이를 기존 API 게이트웨이와 함께 사용하거나, 클라우드 중심, 플랫폼 기반, 자가 서비스 배포를 위해 구식 API 관리를 대체하는 최신 옵션으로 사용할 수 있습니다. AI 게이트웨이는 일반적으로 공개적으로 호스팅되는 LLM으로 나가는 요청에 사용됩니다. 그림 2-2에서 볼 수 있듯이, 호출은 클라이언트에서 AI 게이트웨이로 이동한 다음 LLM(이 경우 OpenAI)으로 나갑니다. 이 시나리오에서 AI 게이트웨이는 강력한 포워딩 프록시로 작용합니다.

AI 게이트웨이는 엔터프라이즈가 AI/LLM 사용을 안전하고 효율적으로 도입하고 확대하는 데 핵심적인 역할을 합니다. 많은 조직들은 OpenAI와 같은 공개 모델을 사용하여 AI 여정을 시작하는데, 이는 유연성과 통합의 용이성 때문입니다. 그러나 도입이 증가함에 따라, 비용, 데이터 프라이버시, 규정 준수에 관한 우려를 해결하기 위해 개인 또는 하이브리드 설정으로 전환하는 경우가 많습니다.


Figure 2-2. Requests to the LLM follow a forwarding proxy model
보안과 프라이버시는 AI 게이트웨이의 기초적 측면입니다. 조직은 민감한 엔터프라이즈 데이터가 공개 LLM에 실수로 노출되지 않도록 해야 합니다. 맞춤형 가드레일을 통해 요청과 응답의 민감한 데이터를 마스킹, 차단 또는 필터링하여 상호작용의 모든 단계에서 데이터를 보호합니다. 또한, 중앙 집중식 자격증명 관리는 비밀 저장소를 사용하여 API 키를 보호하고 역할 기반 접근 제어(RBAC)로 세분화된 접근을 강제합니다. JSON Web Token(JWT) 및 외부 인증 메커니즘과 같은 견고한 인증 정책과 결합하면, 게이트웨이는 승인된 사용자만 AI API에 접근할 수 있도록 보장합니다.

AI 게이트웨이는 운영 가시성을 활성화합니다. 요청 볼륨, 토큰 사용량, 지연시간과 같은 상세 메트릭은 성능 및 비용 관리를 위한 통찰력을 제공합니다. 견고한 로깅 및 모니터링은 규정 준수 보고, 문제 해결, 감사를 용이하게 하며, AI 도입이 규제 요구사항과 부합하도록 보장합니다. OpenTelemetry와 같은 개방형 표준을 통해 기존 관찰성 도구와 통합하면 최종 사용자를 위한 스마트 대시보드 구축의 기존 계획을 보강하는 데 도움이 됩니다.

AI 게이트웨이는 요청/프롬프트의 의미론적 이해를 활용하기 때문에 로드 밸런싱, 페일오버 또는 결과 캐싱에 대해 더 스마트한 결정을 내릴 수 있습니다. 의미론적 인식은 게이트웨이가 조직에서 승인한 방법에 따라 프롬프트에 추가 문맥을 첨부할 수 있게 해줍니다. 예를 들어, AI 게이트웨이는 검색 증강 생성(RAG)을 구현하는 데 도움이 될 수 있으며, 이는 LLM을 더 정확한 응답으로 안내하기 위해 추가 사실이나 조직별 데이터를 프롬프트에 추가하는 메커니즘입니다.많은 조직들이 내부 개발자 플랫폼을 중심으로 개발자를 위한 고도로 자동화된 자가 서비스 워크플로우로 이동하고 있으며, AI 게이트웨이는 이 플랫폼과 호환되어야 합니다. 우리는 이 장의 마지막 섹션에서 이것에 대해 더 다룰 것이지만, 이 통합을 용이하게 하려면 AI 게이트웨이 설계를 고려할 때 다음을 고려해야 합니다:

우려사항의 분리

강력한 기초 프록시

선언적 구성

오픈 소스

우려사항의 분리

AI 게이트웨이 설계는 명확한 우려사항 분리로 시작됩니다. 즉, 게이트웨이는 애플리케이션의 비즈니스 로직과 구별되는 운영 계층으로 작용해야 합니다. 이러한 우려사항들을 분리함으로써, 운영 코드를 지속적으로 수정하지 않으면서 애플리케이션의 기능을 향상시키는 데 집중할 수 있습니다. 예를 들어, 민감한 데이터 마스킹, 입력 검증, 또는 API 할당량 강제는 애플리케이션 내에 포함되지 않고 게이트웨이의 일부로 구현되어야 합니다.

Leverage a Powerful and Flexible Proxy
AI 게이트웨이의 기초로 사용할 프록시 선택은 매우 중요합니다. 프록시는 성능, 구성 가능성, 그리고 성숙도를 갖춰야 합니다. Envoy는 성능, 확장성, 확장성으로 알려져 있습니다. Envoy의 강력한 필터 체인 아키텍처는 속도 제한, 외부 인증(ext-auth), 일반적인 외부 처리(ext-proc)와 같은 기능의 표준 플러그인으로 유연성을 가능하게 합니다. 이러한 기능들은 게이트웨이가 요청과 응답 본문을 조작하여 민감한 데이터를 마스킹하는 것과 같은 복잡한 작업을 처리할 수 있게 해주며, 애플리케이션 코드에 과부하를 주지 않습니다. Envoy를 사용하면 시스템이 성능 있고 확장 가능한 상태를 유지하면서 고급 게이트웨이 기능을 빠르게 개발하고 배포할 수 있습니다.

Declarative Configuration
선언적 구성은 AI 게이트웨이 관리의 모범 사례로, 예측 가능성을 보장하고 인간 오류의 위험을 줄입니다. 선언적 접근 방식을 사용하면 라우팅 규칙, 보안 정책, API 할당량과 같은 게이트웨이 동작의 원하는 상태를 YAML이나 JSON과 같은 구조화된 형식으로 정의할 수 있습니다. 이를 통해 구성을 버전 관리하고, 환경 전체에 일관되게 적용하며, 업데이트를 자동화하기가 더 쉬워집니다. 또한, 선언적 구성은 인프라 코드(IaC) 관행과 원활하게 통합되어 통일되고 반복 가능한 배포 프로세스를 촉진합니다.

Roots in Open Source
AI 게이트웨이를 오픈 소스 기반 위에 구축하면 몇 가지 이점을 얻을 수 있습니다. Envoy와 같은 오픈 소스 기술은 지속적인 혁신, 보안 업데이트, 기능 개발에 기여하는 크고 활발한 커뮤니티와 함께 제공됩니다. 이는 벤더 종속성을 줄이고 엔터프라이즈가 발전하는 기술 요구사항을 앞서나갈 수 있도록 해줍니다. 오픈 소스 소프트웨어를 활용하는 것은 현대의 개발 관행과 부합하며, 특정 엔터프라이즈 요구사항을 충족하도록 게이트웨이를 조정할 수 있는 비용 효율성과 유연성을 제공합니다.IT 조직은 종종 여러 팀을 같은 페이지에 놓기 위해 고민합니다. 팀들은 종종 사일로에서 결정을 내리다 보니 도구의 중복, 통합 도전, 거버넌스 붕괴로 이어지며, 이는 규정 준수 문제를 야기할 수 있습니다. 플랫폼 엔지니어링은 다양한 IT 팀을 통합하여 소프트웨어 전달과 운영 우수성을 간소화하는 데 중요한 역할을 합니다. 애플리케이션 개발자, 보안 전문가, 네트워킹 전문가 등 다양한 IT 팀 간의 협업을 촉진함으로써, 플랫폼 엔지니어링 계획은 마찰을 줄이고, 개발자 생산성을 향상시키며, 규정 준수 표준을 강제하는 것을 목표로 합니다.

GitOps 및 IaC와 같은 자동화 우선 원칙으로 설계된 AI 게이트웨이는 최소한의 방해로 이러한 환경에 통합될 수 있습니다. 이들은 엔터프라이즈가 팀이 이미 사용하는 워크플로우에 AI 기반 통찰력과 오케스트레이션을 포함시킴으로써 플랫폼 노력을 가속화하도록 합니다. 예를 들어, 데이터 과학자와 개발자는 더 효과적으로 협업할 수 있으며, AI 게이트웨이를 사용하여 견고한 보안 및 거버넌스를 유지하면서 모델, API, 인프라에 대한 자가 서비스 접근을 활성화할 수 있습니다. 이러한 접근 방식은 AI 도입을 규모에 맞게 가속화할 뿐만 아니라 팀에 자가 서비스 도구를 제공하고 조직 전체의 정렬을 보장하는 플랫폼 엔지니어링 미션을 강화합니다.

조직들이 AI 및 LLM 사용을 도입함에 따라, AI 게이트웨이는 중요해집니다. 이는 다리 역할을 하여 고급 의미론적 이해를 워크플로우에 짜넣어 비용 급증, 데이터 프라이버시 우려, 규정 준수 복잡성의 위험을 완화합니다—이는 엔터프라이즈 환경의 핵심 문제입니다. 앞서 논의한 모범 사례(우려사항 분리, 강력한 프록시, 선언적 구성, 오픈 소스)로 설계된 AI 게이트웨이는 내부 개발자 포털에 잘 통합되도록 설계되었습니다. 다음 장에서는 이러한 모범 사례로 구축된 AI 게이트웨이 구현을 살펴봅니다.

# Chapter 3. Common AI Gateway Use Cases
이전 장에서는 AI 게이트웨이의 개념을 소개하고 기존 네트워킹이 왜 부족한지를 설명했습니다. 이 장에서는 엔터프라이즈가 AI 게이트웨이를 사용하여 실제 문제를 어떻게 해결할 수 있는지 살펴보겠습니다.

사용 사례를 살펴보면서 예시로 AI 게이트웨이 구현을 참고하겠습니다. 사용자는 Envoy Proxy와 Kubernetes Gateway API를 포함한 최신 아키텍처 위에 구축된 AI 게이트웨이를 선택해야 합니다. Gloo AI Gateway(Cloud Native Computing Foundation—CNCF에 KGateway로 기여 중과 같은)와 같은 게이트웨이는 보안, 관찰성, 제어 및 거버넌스 요구를 해결하면서 AI 애플리케이션 개발을 가속화하는 AI 기능을 제공합니다.

신뢰할 수 있는 AI 게이트웨이를 선택하면 엔터프라이즈가 AI 도입 시 직면하는 주요 우려사항을 해결하는 데 도움이 됩니다:

- 보안 위협 완화
- 기술 역량 격차 해소
- 기존 인프라와의 원활한 통합

이런 배경을 바탕으로 AI 게이트웨이가 엔터프라이즈 AI 도입에 필수적인 주요 사용 사례들을 살펴보겠습니다.

Security and Access Control
보안과 접근 제어는 모든 기술에서 가장 중요한 요소이며, AI 사용 사례도 예외는 아닙니다. AI 게이트웨이는 API 자격증명 관리에서 다중 테넌시 격리에 이르기까지 다양한 기능을 포괄할 수 있습니다.

이 섹션에서는 AI 게이트웨이가 인증, 권한 부여, 접근 정책을 처리하는 보안 계층을 어떻게 제공하여 AI 자원과 LLM 제공자에 대한 접근이 승인된 사용자와 애플리케이션으로만 제한되도록 하는지 살펴보겠습니다.

API Key and Credential Management
모든 LLM 제공자는 API 키를 필요로 하기 때문에, 이를 관리하는 것은 엔터프라이즈 환경에서 중요한 보안 과제를 제시합니다. 애플리케이션 코드나 구성 파일에 API 키를 직접 포함하는 전통적 접근 방식은 자격증명 확산(credential sprawl)을 초래하여 코드 저장소나 구성 누수로 인한 우발적 노출 위험을 증가시킵니다. 조직이 AI 도입을 확장하고 여러 LLM 제공자를 통합할수록 이 문제는 더욱 심화됩니다.

그림 3-1은 AI 게이트웨이가 애플리케이션 코드에서 분리된 인프라 수준에서 자격증명 관리를 중앙화하여 이러한 과제를 어떻게 해결하는지를 보여줍니다. 이를 통해 운영자는 여러 제공자에 대한 API 키를 안전하게 저장하고 관리할 수 있으며, 그림 3-2에 표시된 대로 엔터프라이즈 비밀 관리 시스템과 통합할 수 있습니다.

Figure 3-1. Comparison of direct versus gateway-mediated access to LLMs

Figure 3-2. Integration of enterprise secrets management system with an AI gateway architecture
애플리케이션 팀으로부터 자격증명 관리 책임을 제거함으로써 조직은 일관된 보안 관행을 구현하고, 키 회전 자동화, 그리고 모든 AI 워크로드에 걸친 자격증명 사용에 대한 포괄적인 감사 추적을 유지할 수 있습니다.

Fine-Grained Access Control and Auditing
LLM 자격증명 관리를 중앙화하는 것이 보안 문제의 일부를 해결해 주지만, 조직은 LLM에 대한 접근 제어도 필요합니다. 사용자는 기존 엔터프라이즈 인증 및 권한 부여 메커니즘과 통합할 수 있는 AI 게이트웨이를 선택해야 합니다. 조직이 JWT, 커스텀 토큰, OpenID Connect(OIDC), Lightweight Directory Access Protocol(LDAP) 또는 기타 인증 방법을 사용하든, AI 게이트웨이는 새로운 자격증명을 도입하기보다는 이러한 기존 시스템을 활용할 수 있습니다.

여기서는 JWT를 사용하는 예를 살펴보겠습니다. 다른 인증 방법에도 유사한 원칙이 적용됩니다. JWT는 AI 게이트웨이에서 세분화된 접근 제어를 구현하는 한 가지 방법으로 사용됩니다.

API 키와 달리 JWT는 사용자의 신원, 역할, 권한 및 기타 값을 나타내는 풍부한 클레임(키-값 쌍)을 포함합니다. 이는 세분화되고 유연한 접근 제어 결정을 가능하게 합니다.

다음은 커스텀 클레임을 포함한 JWT의 예입니다:

{
  "iss": "sso.solo.io",
  "sub": "peterj",
  "team": "ai-research",
  "llms": {
    "openai": ["gpt-4", "gpt-3.5-turbo"],
    "anthropic": ["claude-3-opus"]
  }
}
JWT의 클레임을 기반으로 AI 게이트웨이는 여러 수준에서 접근 제어를 시행할 수 있습니다. 몇 가지 예를 살펴보겠습니다.

팀 및 LLM 접근 제어
이 예시는 JWT 클레임의 값에 따라 어떤 모델에 접근할 수 있는지를 제한하거나 팀 격리를 시행하는 방법을 보여줍니다(RBAC 사용). 그림 3-3에 표시된 바와 같이, 예제 구성은 JWT에 team: ai-research 값이 포함된 경우에만 openai-route에 대한 접근을 허용합니다:

apiVersion: gateway.solo.io/v1
kind: RouteOption
metadata:
  name: jwt-ai-research
spec:
  targetRefs:
  - group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: openai-route
  options:
    rbac:
      policies:
        viewer:
          nestedClaimDelimiter: .
          principals:
          - jwtPrincipal:
              claims:
                team: ai-research

Figure 3-3. Using RBAC to allow access to the AI research team and restricting access for the QA team through an AI gateway
유사하게, JWT 클레임을 기반으로 사용자가 접근할 수 있는 모델을 제한하는 구성도 만들 수 있습니다.

Usage control
게이트웨이 운영자는 선언적 구성을 사용하여 그림 3-4에 표시된 것처럼 JWT의 클레임에 기반한 토큰 속도 제한을 적용할 수 있습니다. 아래 예시는 각 JWT의 sub(subject) 클레임에 대해 시간당 500 토큰의 속도 제한을 구성합니다. 이 속도 제한 구성은 AI 게이트웨이가 이를 강제하는 특정 라우트에도 별도로 적용됩니다:

...
    descriptors:
    - key: user-id
      rateLimit:
        requestsPerUnit: 500
        unit: HOUR
    rateLimits:
    - actions:
      - metadata:
          descriptorKey: user-id
          source: DYNAMIC
          default: unknown
          metadataKey:
            key: "envoy.filters.http.jwt_authn"
            path:
            - key: principal
            - key: sub

Figure 3-4. Token rate limiting with per-subject quota from JWT
Securing Multi-Tenancy in AI Workflows
대규모 기업은 일반적으로 AI 기능에 접근해야 하는 여러 비즈니스 유닛, 개발팀 또는 외부 파트너를 보유하고 있습니다. 예를 들어 금융 서비스 회사에서는 자산관리, 소매은행, 보험 등 서로 다른 부서가 각기 다른 목적을 위해 LLM을 사용해야 할 수 있습니다. 각 부서는 고유한 보안 요구사항, 데이터 처리 요구, 예산 제약을 가지고 있습니다.

AI 게이트웨이와 서비스 메쉬와 같은 솔루션을 결합하면 엔터프라이즈 팀은 안전한 멀티테넌시를 구현할 수 있습니다. 이를 통해 조직은 각 테넌트를 격리하면서도 제어와 거버넌스를 유지할 수 있습니다.

예를 들어, 자산관리 부서는 투자 보고서를 분석하기 위해 OpenAI의 GPT-4o 모델을 사용해야 할 수 있고, 소매은행 팀은 고객 서비스 자동화를 위해 다른 모델이나 제공자를 사용할 수 있습니다.

이 접근 방식으로 각 테넌트에 대해 다음을 보장할 수 있습니다:

- 별도의 API 자격증명 및 속도 제한
- 부서별 프롬프트 템플릿 및 구성
- 분리된 데이터 처리 정책
- 개별 비용 센터 및 사용 할당량
- 맞춤형 보안 정책 및 준수 규칙

Data Privacy and Compliance Enforcement (Guardrails)
현대의 엔터프라이즈는 일상적으로 방대한 양의 사적이고 민감한 데이터를 다루며, 엄격한 데이터 준수 및 기밀성 조항을 필요로 합니다. 또한 모든 민감한 데이터는 HIPAA, GDPR 및 캘리포니아 소비자 개인정보 보호법(CCPA)과 같은 적용 가능한 지역 규정을 포함한 규제 및 준수 요구사항을 따라야 합니다.

AI 게이트웨이는 모든 애플리케이션과 AI 모델 및 백엔드 사이를 흐르는 데이터를 검사하고 필터링하며 거버넌스하는 중개자 역할을 합니다. 이 섹션에서는 콘텐츠 필터링과 PII(개인식별정보) 탐지를 포함한 데이터 보호 전략을 엔터프라이즈가 어떻게 구현할 수 있는지 살펴봅니다.

데이터를 보호하기 위해 엔터프라이즈는 가드레일 및 데이터 유출 방지(DLP) 전략을 구현해야 합니다. AI 게이트웨이의 가드레일은 들어오는 요청(엔드유저나 애플리케이션의 프롬프트 또는 쿼리)과 LLM으로부터 나오는 응답 모두에 대해 양방향 보호를 제공합니다.보호 방식은 단순히 응답을 거부하고 미리 정의된 메시지(예: "부적절한 콘텐츠로 인해 거부됨")를 반환하거나, 응답을 마스킹하는 것일 수 있습니다. 마스킹은 민감한 내용을 특정 단어나 문자로 대체하는 것을 포함합니다. 예를 들어 위치를 마스킹하려면 "Italy is a great place to visit"라는 문장은 "[LOCATION] is a great place to visit."처럼 보일 수 있습니다. 또 다른 옵션은 감지된 문자열을 완전히 가려서 어떤 내용이었는지 단서를 주지 않는 방식으로 "XXXXX is a great place to visit."처럼 처리하는 것입니다.

게이트웨이에서 들어오는 요청에 대해 가드레일은 요청이 LLM으로 전송되기 전에 동작합니다. 이를 통해 요청이 LLM에 도달하기 전에 감지 및 자동 차단이 가능하여 데이터 프라이버시 규정을 준수할 수 있습니다. 이 접근법은 민감한 데이터가 외부 AI 서비스로 실수로 공유되는 것을 방지합니다.

응답 측면에서는 LLM 출력이 민감한 정보를 포함하는 엔터프라이즈 데이터를 참고하여 보강될 수 있습니다. 정확한 문맥 제공에는 유용하지만, 민감한 데이터를 호출자에게 그대로 반환하는 것은 바람직하지 않을 수 있습니다. 응답에 대한 가드레일은 PII를 감지하여 마스킹 규칙을 적용하거나 필요한 경우 응답을 완전히 거부하고 "Rejected"와 같은 미리 정의된 메시지로 대체할 수 있습니다. 이렇게 하면 LLM이 내부 데이터 소스를 통합하더라도 보호 대상 정보가 안전하게 유지됩니다. 

그림 F3-5는 입력 및 출력 가드레일이 적용된 비정제(unsanitized) 및 정제(sanitized)된 입출력의 흐름을 보여줍니다.

가드레일 시스템(예: Microsoft Presidio)은 일반적으로 다음 방법 중 하나 이상을 사용하여 양방향 콘텐츠를 평가하고 필터링합니다:

- 내장 패턴
신용카드 번호, 주민등록번호, 이메일, 전화번호 등을 탐지하는 데 사용될 수 있습니다.

- 정규식(Regular expressions)
일치하는 문자열을 차단하거나 마스킹하는 데 사용할 수 있습니다.

- 외부 중재(External moderation)
요청/응답을 마스킹하거나 거부해야 하는지 결정하는 데 사용할 수 있습니다. 이는 OpenAI의 omni-moderation 같은 외부 모델이나 사용자 정의 웹훅일 수 있습니다.

직접적인 콘텐츠 필터링을 넘어서, 조직은 프롬프트 템플릿을 통해 프라이버시 및 규정 준수를 더욱 강화할 수 있습니다. 이러한 템플릿은 시스템 프롬프트나 추가 컨텍스트를 자동으로 사용자 쿼리에 첨부하도록 허용하여 모든 LLM 상호작용에 대한 일관된 기본 규칙을 설정합니다.

예를 들어 그림 3-6에서는 사용자 쿼리에 감정 분석을 수행하고 그 결과를 프랑스어 단어 한 개로 응답하라는 시스템 프롬프트를 자동으로 첨부합니다. AI 게이트웨이를 통해 이 기능을 제공하면 조직은 표준화된 시스템 프롬프트를 정의하고 모든 클라이언트 요청에 자동으로 적용할 수 있어, 개별 애플리케이션이 각 쿼리마다 이러한 규칙을 명시하지 않아도 프라이버시 정책 및 규정 준수 요구사항을 준수할 수 있습니다. 이러한 템플릿 방식은 애플리케이션이 LLM과 상호작용하는 방식을 표준화하는 통합 거버넌스 계층을 만듭니다.

Figure 3-6. Composing user input and predefined system template allowing standardized LLM interaction

Performance and Reliability
AI 워크로드는 가변적인 응답 시간 처리부터 높은 동시성 시나리오 관리를 포함하여 성능 및 신뢰성 관리 측면에서 고유한 과제를 제시합니다. AI 게이트웨이는 일관

Performance Metrics and User Experience
AI 게이트웨이를 통해 AI 워크로드를 관리할 때 핵심 성능 메트릭을 이해하고 최적화하는 것은 서비스 신뢰성에 매우 중요합니다. 사용자 경험에 영향을 주는 주요 메트릭은 지연시간 기반이며, 특히 응답이 토큰 단위로 생성되는 스트리밍 시나리오에서 중요합니다. 기본적으로 LLM 제공자는 전체 응답이 생성된 후에 응답을 반환합니다.

게이트웨이 성능을 모니터링하는 데 중요한 두 가지 메트릭은 TTFT(챕터 1에서 언급됨)와 출력 토큰당 시간(TPOT)입니다.

- TTFT는 쿼리 제출부터 첫 번째 생성 토큰까지의 초기 응답 시간을 측정합니다. 이 메트릭은 실시간 애플리케이션(예: 챗봇)에서 중요하며, 사용자는 즉각적인 응답을 기대합니다. AI 게이트웨이의 트래픽 관리 결정은 라우팅 및 부하 분산을 최적화하여 TTFT에 큰 영향을 미칠 수 있습니다.

- TPOT은 이후 토큰들이 생성되는 속도를 측정합니다. 스트리밍 응답의 경우, TPOT은 인간의 읽기 속도(텍스트 복잡성과 읽기 속도에 따라 대략 초당 5–6 토큰)와 일치해야 부드러운 사용자 경험을 보장합니다. AI 게이트웨이는 지능형 로드밸런싱과 자원 할당을 통해 TPOT을 최적화할 수 있습니다.

총 지연시간을 계산하려면 TTFT에 TPOT과 출력 토큰 수의 곱을 더하면 됩니다(그림 3-7 참조).

예상 지연시간을 추정할 때, 상호작용 중 AI 에이전트가 많은 작업을 수행하고 여러 LLM 호출을 처리하고 있다면 응답 시간이 지연될 수 있음을 염두에 두어야 합니다. 이러한 이유로 성능 테스트에서는 예상 프로덕션 조건을 반영할 만큼 충분히 무거운 워크로드를 생성하는 것이 합리적입니다.

Figure 3-7. Latency components in LLM response generation

Traffic Management and Scaling for AI Workloads

강력한 트래픽 관리는 신뢰할 수 있고 비용 효율적인 서비스를 유지하는 데 매우 중요합니다. AI 게이트웨이는 요청을 효율적으로 라우팅하고, 엔드포인트 간 로드를 균형 있게 분산하며, 정책을 적용하면서 최적의 성능과 리소스 활용을 보장해야 합니다.

예를 들어 Gloo AI Gateway(또는 KGateway로 알려짐)는 Envoy 프록시의 트래픽 관리 기능을 활용하여 여러 팀이 실행하고 서로 다른 제공자에 접근하는 AI 워크로드를 처리합니다.

이 시나리오에서 Gloo AI Gateway는 트래픽 흐름을 최적화하기 위해 분산 및 글로벌 로드밸런싱 전략을 모두 구현합니다. 활성 상태 검사(active health checking)와 지능형 로드밸런싱 알고리즘을 사용하여 현재 성능, 가용성, API 할당량을 기반으로 모델 엔드포인트로 요청을 라우팅합니다. 글로벌 수준에서는 비용, 성능, 지리적 위치 등을 기반으로 특정 모델 제공자나 엔드포인트를 선호하도록 조직 전체의 트래픽 정책을 시행할 수 있습니다.

라우팅 및 트래픽 관리는 다양한 요청 속성에 기반합니다. 예를 들어 조직은 다음과 같은 기준에 따라 요청을 서로 다른 모델 엔드포인트로 유도하는 라우팅 규칙을 구성할 수 있습니다:

- 쿼리의 복잡성 또는 우선순위를 나타내는 요청 헤더
- 모델 기능이나 버전을 지정하는 경로 매개변수
- 라우팅 선호도를 결정하는 팀 또는 애플리케이션 식별자
- 비용과 성능을 최적화하는 리소스 활용 패턴
- 제공자 토큰 사용 한도(예: 토큰 한도 초과 시 다른 모델로 라우팅)

상태 검사 및 이상치 감지 메커니즘은 엔드포인트 상태를 지속적으로 모니터링하여 열화된 인스턴스를 로드밸런싱 풀에서 자동으로 제거합니다. 장애 발생 시 AI 게이트웨이는 자동으로 페일오버하여 백업 시스템으로 전환할 수 있습니다.

이 예에서 Gloo AI Gateway는 우선순위를 두고 싶은 LLM 제공자의 모델들에 대한 페일오버도 처리할 수 있습니다. 한 제공자의 주 모델이 사용 불가하거나 느려지거나 문제가 발생하면, AI 게이트웨이는 동일 제공자 또는 다른 제공자의 백업 모델로 빠르게 전환합니다. 이를 통해 서비스가 중단 없이 계속 운영될 수 있습니다.

Integration of Hybrid AI Solutions
퍼블릭 클라우드 기반의 LLM은 개념 증명 작업을 시작하기 가장 쉬운 방법일 수 있지만, 조직들은 종종 서로 다른 인프라에 배포된 퍼블릭 및 프라이빗 모델을 모두 활용해야 합니다. 이러한 하이브리드 접근 방식은 다양한 퍼블릭 모델의 고급 기능과 자체 인프라에서 실행되는 프라이빗 모델이 제공하는 보안 및 통제를 균형 있게 활용할 수 있게 해줍니다. AI 게이트웨이는 이 아키텍처를 관리하는 오케스트레이션 계층으로서 이상적인 위치에 있어 모든 종류의 모델에 대해 통합된 라우팅, 보안 및 거버넌스를 제공합니다.

예를 들어, 엔터프라이즈는 일반 용도의 작업에 GPT-4o 같은 퍼블릭 모델을 사용하면서, 민감한 데이터가 포함된 쿼리는 자체 인프라에서 실행되는 맞춤형 또는 파인튜닝된 모델로 라우팅할 수 있습니다. AI 게이트웨이는 콘텐츠 유형, 보안 요구사항, 요청의 특정 헤더 등 다양한 요소를 기준으로 이러한 라우팅 결정을 내릴 수 있습니다. 이러한 지능형 라우팅은 각 쿼리가 가장 적합한 모델로 전달되도록 하면서 전체 인프라와 AI 자원 전반에 일관된 보안 정책과 모니터링을 유지합니다.

Optimization and Monitoring
조직 내에서 AI 사용이 증가함에 따라 사용 패턴을 이해하고, 비용을 제어하며, 시스템 성능에 대한 가시성을 유지하는 것이 점점 더 중요해집니다.

이 섹션에서는 AI 게이트웨이가 의미론적 캐싱(semantic caching) 및 지능형 라우팅과 같은 기능을 통해 엔터프라이즈의 AI 투자를 최적화하고, 시스템 성능을 유지·향상시키는 데 필요한 관찰성(observability)을 제공하는 방법을 살펴봅니다.

Cost Optimization and Budget Controls
의미론적 캐싱은 LLM 작업에서 계산 비용을 줄이고 응답 시간을 개선하는 접근 방식입니다. 정확한 일치에 의존하는 전통적 캐싱과 달리, 의미론적 캐싱은 쿼리의 의도를 이해하여 질문이 다르게 표현되더라도 이전 응답을 재사용할 수 있게 합니다.

LLM이 생성하는 모든 응답은 비용이 발생하므로 의미론적 캐시의 긍정적 적중(캐시 히트)이 발생할 때마다 LLM 제공자에 대한 비용 발생 호출 수가 줄어듭니다. 의미론적 캐싱은 일반적으로 LLM이 생성하는 응답에 비해 응답 시간을 개선하므로 사용자 경험도 향상됩니다.

예를 들어 고객 문의를 처리하는 고객 서비스 AI를 생각해봅시다. 한 고객이 "전자제품 반품 정책이 어떻게 되나요?"라고 묻고, 이후 다른 고객이 "내 노트북을 반품하려면 며칠 이내에 해야 하나요?"라고 묻는다면, 전통적 캐시는 이를 서로 다른 쿼리로 간주하여 각각 별도의 LLM 호출을 수행할 것입니다. 그러나 의미론적 캐싱은 이 둘의 의도가 유사함을 인식하고 첫 번째 쿼리의 캐시된 응답을 두 번째 고객에게 제공하여 시간과 비용을 절감할 수 있습니다(그림 3-8 참조).


Figure 3-8. Using a vector database to retrieve semantically similar requests
It’s important to note that implementing semantic caching itself requires embedding models or smaller LLMs at the gateway level to analyze query intent, which introduces its own computational overhead and complexity. This means organizations need to carefully evaluate the trade-off between gateway costs and potential savings from reduced LLM calls.

The cost savings of semantic caching can be substantial. If we continue with the previous example, let’s say there are 100,000 customer queries per month using GPT-4, with an average cost of $0.03 per query. Without semantic caching, this would cost $3,000 a month. If semantic caching can handle 60% of queries (a conservative estimate for common customer service scenarios), the monthly cost drops to $1,200—a savings of $1,800. Over the course of a year, this represents $21,600 in direct LLM cost reductions, not including the additional savings from reduced computational resources and improved response times.

Another optimization that impacts latency, and improves perceived responsiveness and TTFT metrics is streaming mode. In streaming mode, the LLM sends tokens to the client as they are generated, rather than waiting for the entire response to be completed, allowing users to see the response build progressively in real time. Streaming mode is provided by the LLM backend and must be implemented and enabled in the AI gateway.

Monitoring and Observability for AI Pipelines
Just like for any application, good monitoring and observability are crucial when running applications that use AI and LLMs. The operating unit of measurement and cost is tokens. They are the units that organizations are charged for. Both input and output tokens matter: the input tokens are the prompts and queries that are sent to the LLM and the output tokens represent the response from the LLM. The costs for both are typically represented per one million tokens and differ by LLM provider.

Depending on the AI provider and the model you’re using, you should expect the cost to be anywhere from a few cents to a few dollars per million input tokens, while the output token responses will typically be a few times more expensive.

With varying costs for tokens, it’s clear why comprehensive monitoring is important. An efficient observability system enables organizations to identify issues early, allowing them to optimize model usage, track costs, and ensure that budgets are not exceeded.

In “Performance Metrics and User Experience”, we talked about the performance metrics TTFT and TPOT. Here are some other metrics, grouped into categories, which can provide valuable insights into your AI applications:

Resource utilization

Token consumption patterns by model and endpoint

Request distribution across different LLM providers

Semantic cache hit rates and efficiency metrics

Concurrent request patterns and queue depths

Quality metrics

Completion success rates

Error rates and types (context length, content filtering, timeout)

Response coherence scores

Hallucination detection rates

Financial metrics

Cost per request by model

Token utilization efficiency

Budget adherence by team/project

Cost anomaly detection

User experience

TTFT

TPOT

TPS

Total request latency distribution

Conclusion
AI gateways serve as a critical foundation for enterprise AI adoption by providing comprehensive solutions for security, performance optimization, and observability challenges. Features like centralized credential management, semantic caching, and detailed monitoring capabilities enable organizations to safely scale their AI implementations while maintaining control over costs, security, and compliance requirements.

In the next chapter, we’ll look ahead at how these foundational capabilities will evolve in the future.

# Chapter 4. The Future of Enterprise AI
AI makes for an exciting and innovative period of time in information technology. While many organizations are leveraging AI to either optimize and make business workflows more efficient, or to innovate and provide better user experiences, we believe more and more use cases will take advantage of this technology. Scaling AI use cases and adoption will require specialized tooling that is “AI-aware” such as an AI gateway as discussed in this report. Let’s see how the future of AI in enterprises contributes to the challenges of scale, security, observability, and compliance.

Improvement of Models, Continued Adoption
We expect that enterprises will continue to innovate and invest in AI and LLM-powered use cases. LLMs are helping to automate routine tasks, such as system monitoring, incident response, and infrastructure optimization, enabling faster resolution times and improved efficiency. For security and threat detection, these models enhance the ability to identify patterns, detect anomalies, and respond to cyber threats in real time. Customer service and support are also undergoing a shift, with AI-powered chatbots and virtual assistants delivering personalized, around-the-clock support that improves customer satisfaction and reduces operational costs. These investments are driving strategic business outcomes and helping businesses maintain a competitive advantage.

Alongside the continued expansion of AI-backed use cases, we expect the underlying models to become more powerful and cheaper. Over the next year, we can expect significant advancements in LLMs across several key areas. Context windows are expanding dramatically, with some models like Anthropic’s Claude already reaching 100,000 tokens, and Google’s Gemini supporting up to two million tokens. These numbers may be outdated by the time you read this, but the fact remains that these models will continue to extend context capabilities, which will allow models to analyze even larger amounts of content and datasets in a single interaction. Reasoning capabilities are also improving rapidly, as seen with OpenAI’s o1 and o3 models, which emphasize enhanced logical consistency, multistep problem solving, and contextual understanding. Lastly, the cost to train these models will likely drop as seen with the claims of the recent release of DeepSeek-R1. This progression will make AI more adept at addressing complex, nuanced queries across enterprise domains.

Adopting Internal Models
Enterprises are increasingly exploring the potential of running and training their own language models, leveraging small language models (SLMs) or open source options like Llama, Mistral, and others. This trend is driven by the need for greater control over data privacy, cost efficiency, and customization. SLMs and open source models enable organizations to tailor models to their specific domain requirements while reducing dependency on third-party vendors. These smaller, fine-tuned models can deliver competitive performance for many use cases, such as document summarization, customer support, and predictive analytics, without requiring the massive infrastructure and resources of larger LLMs.

Kubernetes plays a critical role in operationalizing these AI workloads, as it is already the strategic runtime container platform for most enterprises. Kubernetes provides the scalability, resource management, and orchestration necessary for running inference workloads efficiently across distributed environments. By containerizing model training and inference tasks, organizations can integrate AI into their existing continuous integration, continuous delivery (CI/CD) pipelines and manage workloads at scale. Moreover, tools like KServe enable the automation of model training, deployment, and monitoring, allowing teams to adopt AI in a cost-effective and production-ready manner. This synergy between Kubernetes and enterprise AI adoption aligns nicely with platform engineering efforts currently underway at most organizations to provide a consistent user experience, self-service, and improved compliance.

Better Control over Model Inference with Extensions to Gateway API
The Kubernetes community is actively advancing its support for inference workloads through initiatives like the LLM Instance Gateway proposal, developed by the WG Serving working group and sponsored by SIG Apps. Inference workloads in this context means running models that have already been trained on Kubernetes, usually utilizing some specialized hardware like GPUs. This proposal introduces specialized load-balancing algorithms, custom resource definitions (CRDs), and controllers tailored to calling LLM services, enabling efficient routing and resource management for requests going to running models, while taking into account specialized accelerators and fine-tuned models. By multiplexing LLM services over shared compute pools, the initiative aims to optimize resource utilization and improve inference performance, addressing the unique challenges of deploying AI workloads in Kubernetes environments. These efforts align with Kubernetes’ role as the strategic container runtime for modern applications, further streamlining the integration of cutting-edge AI capabilities into enterprise infrastructure while maintaining scalability, efficiency, and ease of operation.

Autonomy and Agentic Workflows
The future of AI is focused on transforming LLMs from reactive chatbots into proactive, autonomous agents capable of sophisticated multistep planning and execution. Agentic AI systems—those that can act independently and make decisions on behalf of users—represent a significant evolution beyond current capabilities. Unlike traditional LLMs, which respond passively to user prompts, these next-generation AI agents will operate with a degree of autonomy, dynamically analyzing goals, prioritizing tasks, and executing them across various domains without constant human oversight. For example, instead of simply answering a query about generating a report, an AI agent could autonomously collect relevant data, synthesize insights, format the document, and even schedule a presentation for stakeholders.

These advancements hinge on integrating LLMs with reinforcement learning, planning algorithms, and external systems. By leveraging tools and APIs, autonomous AI agents will interact seamlessly with other software, databases, and Internet of Things (IoT) devices, enabling them to perform actions like automating business workflows, managing cloud infrastructure, or even running complex simulations. Frameworks such as CrewAI or LangChain exemplify early steps in this direction, allowing AI agents to decompose high-level goals into actionable subtasks and execute them iteratively. As these agents become more sophisticated, they could revolutionize industries ranging from customer service to robotics, ushering in a new era of proactive, goal-driven AI systems that extend far beyond traditional conversational capabilities.

Next Steps
As enterprises continue to integrate AI into their operations, the complexities of managing AI workloads—particularly in areas like security, observability, compliance, and scalability—are becoming increasingly evident. To address these challenges, tools like Gloo AI Gateway (KGateway) and Portkey AI Gateway have emerged as essential components in the enterprise AI ecosystem. By implementing an AI gateway, enterprises can establish a robust framework that not only accelerates AI innovation, but also ensures that AI applications are deployed securely, efficiently, and at scale. This positions organizations to effectively harness the transformative potential of AI while maintaining control over operational complexities.

About the Authors
Christian Posta (@christianposta) is VP, Global Field CTO at Solo.io. He is the author of Istio in Action (Manning), as well as many other books on cloud native architecture, and is well known in the cloud native community for being a speaker, blogger, and contributor to various open source projects in the service mesh and cloud native ecosystem (Istio, Kubernetes, et al.). Christian has spent time at government and commercial enterprises, as well as web-scale companies, and now helps organizations create and deploy large-scale, cloud native, resilient, distributed architectures. He enjoys mentoring, training, and leading teams to be successful with distributed systems concepts, microservices, DevOps, and cloud native application design.

Peter Jausovec is a senior principal technical marketing manager at Solo.io with over 17 years of experience spanning software development, QA, and engineering leadership. A recognized expert in cloud native technologies, he specializes in Kubernetes and Istio. Peter authored the book Cloud Native (O’Reilly) and pioneered several industry-leading initiatives, including developing comprehensive Envoy proxy and Istio training courses that have educated over 15,000 learners. He created the first-ever Istio certification program, now the Istio Certified Associate exam under the Linux Foundation. At Solo.io, Peter has been instrumental in shaping next-generation technologies, including prototyping the AI Gateway.