# Chapter 1. AI도입이 기업의 성공에 필수적인 이유

이 장에서는 AI 도입이 엔터프라이즈 성공에 필수적이 된 이유를 살펴보고, 조직들이 비즈니스 기능의 다양한 분야에서 AI를 활용하여 변혁과 경쟁 우위를 이루는 방법을 살펴볼 것입니다.

인공지능은 지난 몇 년간 화제의 중심이 되었습니다. 엔터프라이즈 기술 환경을 근본적으로 변화시키고 있습니다. 과거의 기술 혁명들—문서 작성 및 워크플로우를 획기적으로 변경한 워드프로세서, 조직의 고객 관계 관리 방식을 변혁한 CRM 시스템, 재무 운영을 혁신한 회계 소프트웨어—을 바탕으로 AI는 새로운 차원의 변화를 도입하고 있습니다. 이전의 혁신 기술들이 각각 특정 비즈니스 기능을 변혁했다면, AI의 영향은 매우 광범위하여 고객 경험 향상, 운영 효율화, 그리고 조직 내 거의 모든 부서와 프로세스에서의 혁신을 동시에 추진하고 있습니다.

최근 연구 데이터는 AI 도입의 영향을 보여줍니다. Google Cloud가 실시한 2,500명의 선임 비즈니스 리더를 대상으로 한 포괄적인 글로벌 설문조사에 따르면, 프로덕션 환경에서 AI를 사용하는 74%의 엔터프라이즈가 첫 해 내에 이미 투자 수익률(ROI)을 확인하고 있습니다. 더욱이 AI로부터 수익 성장을 경험하는 조직의 86%는 연간 수익에서 6% 이상의 이득을 보고하고 있습니다.

유사하게, CIO Tech 설문조사에서는 향후 1년 내에 70%의 엔터프라이즈가 향후 3~5년 내에 비즈니스를 변혁할 AI 도구를 사용할 것으로 예상하여 AI 지출을 증가시킬 계획임을 발견했습니다.

이러한 통계는 AI 도입이 단순한 점진적 개선을 이루는 것이 아니라 주요 영역에 걸쳐 비즈니스 변혁을 가능하게 하고 있음을 강조합니다. 아래 통계는 모두 Google Cloud 설문조사로부터 얻었습니다:

    Customer experience
    
    AI는 조직이 고객과 상호작용하는 방식을 향상시킵니다. Google Cloud 설문조사에 따르면 AI를 통해 사용자 경험을 개선하는 85%의 기업이 사용자 참여도의 측정 가능한 증가를 보고하고 있습니다. 챗봇에서부터 맞춤형 추천에 이르기까지 AI는 고객 여정을 재편하고 있습니다.
    
    Productivity
    
    AI로부터 생산성 개선을 보고하는 엔터프라이즈 중 45%는 직원 생산성이 2배 이상 증가했다고 언급합니다. 이는 AI가 반복적인 작업을 자동화하고 업무 수행 방식을 변혁할 수 있는 능력을 강조합니다.
    
    Business growth
    
    AI는 여러 분야에서 성장을 촉진하고 있으며, 63%의 조직이 AI 도입으로 인한 직접적인 비즈니스 개선을 보고하고 있습니다:
    
    - 45%의 조직은 AI 배포 후 엔지니어링 및 개발자 생산성이 최소 2배 이상 증가했다고 봅니다.
    
    - 84%의 조직은 기술 팀의 가속화와 6개월 이내에 AI 사용 사례를 프로덕션으로 이동할 수 있게 되었다고 보고합니다.
    
    - 70%의 조직은 더 빠른 인사이트 도출 시간을 보고하며, 59%는 핵심 IT 프로세스 및 인프라의 정확도를 개선했습니다.
    
    Security
    
    AI를 통해 보안 태세를 개선한 조직의 56% 중 82%는 더 나은 위협 탐지를 언급하며, 71%는 사건 대응 시간 단축을 보고합니다.

개념 증명에서 프로덕션 시스템으로의 전환은 AI 도입의 분수령을 표시합니다. 오늘날 84%의 조직은 아이디어에서 프로덕션까지 6개월 이내에 AI 사용 사례를 배포할 수 있으며, 이는 이전 기술 혁신과 비교했을 때 놀라운 가속화입니다. 이전 기술들이 특정 비즈니스 기능을 변혁했던 것과 달리, 이러한 빠른 배포 능력은 AI가 인프라와 핵심 프로세스에서부터 고객 대면 서비스와 비즈니스 전략에 이르기까지 조직의 전체 운영 환경에 걸쳐 동시에 변화를 이끌 수 있게 합니다. 이러한 다중 영역에 걸친 수평적 변혁은 기술이 엔터프라이즈를 재편하는 방식의 근본적인 변화를 나타냅니다.

AI 도입은 더 이상 경쟁 관련성을 유지하려는 조직에게 선택 사항이 아닙니다. 초기 AI 도입 기업들은 AI 솔루션 구현에 대한 실무 경험을 축적하고, 팀 전체에 걸쳐 깊은 AI 전문성을 개발하고, AI 통합을 최적화하도록 워크플로우를 재편성하고, AI 모델을 지속적으로 개선하는 평가 피드백 시스템(평가)을 만들어 경쟁 우위를 창출하고 있습니다.

규모에 맞는 AI 구현은 아키텍트와 플랫폼 엔지니어에게 상당한 기술적 과제를 제시합니다. 개념 증명에서 프로덕션으로 이동하는 조직들은 보안, 거버넌스, 확장성 관련 복잡한 요구사항에 직면합니다. AI를 성공적으로 배포하려면 다음을 수행할 수 있는 견고한 기술적 기초를 구축해야 합니다:

- 여러 AI 공급자에 걸쳐 민감한 데이터 및 API 자격증명 보안

- 규모에 맞는 리소스 소비 관리 및 최적화

- 일관된 성능 및 안정성 보장

- 조직 전체의 규정 준수 및 거버넌스 유지

- 기존 시스템 및 워크플로우와의 원활한 통합 실현

이러한 기술적 과제를 성공적으로 해결하면서 빠른 배포와 장기 전략의 균형을 맞추는 조직들은 AI 기반 세계에서의 성공을 위한 위치를 확보하게 될 것입니다.

## Common AI Use Cases
빅데이터 아키텍처의 등장으로 촉진되어 AI 기술은 마침내 학술 연구 센터에서 주류로 이동했습니다. 대규모 언어 모델(LLM)과 같은 새로운 도구의 부상은 팀이 협업하는 방식부터 기업이 데이터를 분석하는 방식까지 모든 것을 변혁하고 있습니다.

    Note
    
    LM은 광대한 양의 텍스트 데이터로 훈련된 인공지능 시스템으로, 인간과 같은 텍스트를 이해하고 생성합니다. LLM의 '대규모(Large)'는 훈련 데이터의 크기와 모델의 매개변수(그 동작을 결정하는 수학적 값)의 크기를 모두 나타냅니다. LLM은 훈련 데이터와 문맥을 바탕으로 수열에서 가장 가능성 있는 다음 단어를 예측하여 작동합니다. 문맥을 이해하고 질문에 답하고 텍스트를 작성할 수 있는 자동완성 시스템이라고 생각하면 됩니다. LLM의 주요 사례로는 GPT-4, Claude, Llama가 있습니다.

오늘날 기업들이 AI를 활용하는 주요 방식들을 살펴보겠습니다. 새로운 사용 사례가 정기적으로 등장한다는 점을 유념하면서 말입니다. 성공의 핵심은 AI가 귀사에 가장 큰 가치를 제공할 수 있는 분야를 파악하고, 인간의 능력을 대체하기보다는 향상시키는 방식으로 이를 구현하는 것입니다.

### Enterprise Productivity and Knowledge Work

AI는 데이터로부터 힘을 얻습니다. 데이터는 AI에서 이중의 역할을 수행합니다: LLM이 훈련되는 기초를 형성하고 그들의 작동을 위한 중요한 문맥으로 기능합니다. 금융 분석가들은 AI를 사용하여 분기별 보고서와 시장 동향에서 통찰력을 빠르게 추출하여, 수시간의 수작업 분석을 수분의 자동화된 처리로 변환합니다. 마케팅 팀은 AI를 활용하여 캠페인 성과 데이터를 분석하고 콘텐츠 변형을 생성합니다. 연구팀은 AI를 사용하여 학술 논문을 요약하고 방대한 발행물 데이터베이스에서 관련 연구를 식별합니다.

LLM은 문서 분석 및 요약, 보고서 생성 및 데이터 시각화, 시장 조사 및 동향 분석, 이메일 관리 및 응답 초안 작성, 회의 요약 및 조치 항목 추출 등과 같은 작업에 강력합니다.

### Customer Experience and Engagement

고객 서비스는 조직들이 AI 구현에서 즉각적인 ROI를 보는 최상위 영역 중 하나입니다.

현대의 AI 기반 고객 서비스 시스템은 단순히 기본 질문에 답하는 것 이상을 수행합니다. 고객이 타이핑하든 말하든 간에 자연어를 이해하고 처리하여 의도를 파악합니다. 대화 흐름은 모든 과거 상호작용, 구매 이력, 계정 세부 정보로 풍부하고 개인화됩니다. 복잡한 문제가 발생하면 이러한 시스템은 고객을 전문가에게 지능적으로 라우팅하는 방법을 알고 있으며 AI가 생성한 문맥, 실시간 통찰력, 응답 제안을 제공합니다.

### Business Process Automation and Decision Support

AI가 핵심 비즈니스 프로세스에 미치는 영향은 이 기술이 단순한 스크립트된 자동화를 어떻게 초월하는지 보여줍니다. 현대 AI는 의사결정에 있어 전략적 파트너로 기능합니다. 단순히 라우팅 작업을 처리하는 것이 아니라 팀이 더 나은 의사결정을 내리도록 돕습니다. 방대한 양의 데이터를 즉시 처리하고 패턴을 감지하며 최적화를 제안할 수 있는 조수라고 할 수 있습니다.

비즈니스 프로세스 자동화 및 의사결정 지원에서 AI의 일반적인 응용 분야는 다음과 같습니다:

- 자동화된 문서 처리 및 데이터 추출
- 지능형 워크플로우 라우팅
- 사기 탐지 및 위험 평가
- 공급망 최적화
- 수요 예측
- 자원 할당
- 품질 관리 및 이상 탐지
- 예측 유지보수

These systems can process vast amounts of data to identify patterns, make predictions, and suggest optimizations that would be impossible for humans to discover manually.

### Software Development and Engineering

In software development and engineering, AI can enhance human creativity and expertise, and improve developer experience by reducing repetitive tasks, which allows developers to spend more time focused on the business logic. However it has not (yet) reached or surpassed humans, and it hasn’t eliminated the need for human oversight. Development teams now work alongside AI tools that can suggest code improvements, automate testing processes, and help maintain system health. Working with an AI coding assistant is mind-blowing. It’s like having another developer looking over your shoulder 24/7, but one that can instantly recall code patterns and best practices.

AI-powered code-generation tools and assistants such as Copilot from Github, v0 by Vercel, Bolt.new from StackBlitz, and even out-of-the-box models from OpenAI or Anthropic are invaluable to developers and can:

- Generate code snippets and complete functions

- Suggest optimizations and identify potential bugs

- Automate testing processes

- Help maintain documentation

- Assist with code reviews

- Generate API documentation

- Create and optimize database queries

- Debug complex issues

### Sales and Marketing

Organizations are leveraging AI for sophisticated customer segmentation and targeting, while using content personalization to deliver tailored experiences at scale.

AI systems excel at lead scoring and qualification, helping sales teams focus their efforts on the most promising opportunities. Marketing teams benefit from AI-driven campaign optimization and comprehensive social media monitoring and analysis, while also using AI to generate marketing copy and analyze A/B tests more effectively.

The technology also helps prevent customer churn through predictive analytics and optimizes conversion rates through intelligent customer behavior analysis. Together, these AI-powered tools enable teams to create more effective campaigns, identify promising leads, and optimize their marketing spend with unprecedented precision.

### Health Care

Health care providers use AI for sophisticated medical image analysis, more accurate disease diagnosis, and personalized treatment planning.

The technology enables continuous patient monitoring and automated drug interaction checking, while streamlining health care records management and appointment scheduling. AI also facilitates better matching of patients with appropriate clinical trials and enables comprehensive population health analysis.

AI allows health care providers to offer better patient care while simultaneously reducing costs and administrative burdens, ultimately leading to more efficient and effective health care systems.

## The Challenges of Implementing AI and LLMs
As organizations move from experimental AI projects to production deployments, they encounter complex challenges that traditional networking infrastructure wasn’t designed to address. Understanding these challenges is crucial for building effective, secure, and governable AI systems.

### Security and Access Control
The most critical security challenges that organizations face when adopting AI center around data privacy and provider trust. Organizations must consider the risk of their proprietary or sensitive information being incorporated into training datasets or being exposed through provider breaches. These concerns are particularly acute given that compromised provider systems could potentially expose not just the raw data, but also queries and responses that might reveal organizational insights or strategies.

Beyond these fundamental concerns, organizations must also tackle the practical challenges of managing access to AI providers securely and efficiently. Unlike traditional APIs, AI services require dynamic access control that can adapt to varying usage patterns and security requirements. Traditional networking tools like routers and firewalls fall short, as they often lack provisions to manage credentials or semantically understand data in transit properly.

Organizations frequently struggle with API key proliferation, where keys might accidentally be hardcoded into applications or exposed in configuration files. This challenge is compounded by inconsistent access controls across teams and applications, making it difficult to maintain security standards. Additionally, organizations must navigate complex regulatory data compliance requirements for personally identifiable information (PII), the Health Insurance Portability and Accountability Act (HIPAA), and the General Data Protection Regulation (GDPR), which traditional security tools aren’t equipped to handle due to their lack of semantic understanding.

### Prompt Management and Data Protection
When employees interact with LLMs, they might include sensitive company or customer information in their prompts as the context to the LLM. Similarly, the LLMs might inadvertently include sensitive information in their responses to the users. This creates a significant security challenge: how do you prevent unauthorized data exposure while still allowing productive AI use?

Organizations need comprehensive prompt management capabilities that include robust guardrails for detecting and masking sensitive information. These systems must provide consistent prompt augmentation across all interactions while maintaining standardized policies for prompt structure and content. Similar to how web application firewalls (WAFs) protect traditional web applications, AI systems require specialized input validation and output constraints to ensure data security and compliance.

### Consumption Control and Resource Optimization
LLM providers like OpenAI, Anthropic, and others make their models available through REST APIs. At a technical level, using an LLM is similar to making any other API call—you send a request with your text and receive the model’s response. This consumption-based nature of LLMs creates unique challenges for cost management and resource optimization. Organizations struggle with attributing costs across different teams and projects, especially as usage patterns vary significantly. This requires sophisticated systems for tracking and controlling resource consumption through token-weighted controls.

Semantic caching presents a particular opportunity for optimization, as many similar queries can be served from cache rather than making redundant API calls. Organizations must also implement intelligent model selection strategies, choosing the most cost-effective model for each use case while maintaining quality standards.

### Resilience and Performance
Traditional load-balancing strategies fall short in AI systems because they don’t account for the semantic nature of queries or key performance metrics like latency (time to first token, or TTFT) and throughput (tokens per second, or TPS). Organizations need intelligent, context-aware systems that can effectively cache similar queries and distribute workloads based on semantic understanding rather than simple request patterns.

Failover mechanisms in AI systems must be more sophisticated than in traditional applications, as organizations often need to switch between different AI providers or models based on performance, cost, or specific use case requirements, while maintaining consistent TTFT and TPS. Cost management adds another layer of complexity, as LLMs operate on a pay-as-you-use basis, where costs are calculated based on tokens, individual units of text that LLMs process. For example, while “dog” counts as one token, longer words like “companionship” might be broken into multiple tokens. This token-based pricing model means that failover decisions must balance performance requirements with cost implications.

These challenges cannot be effectively addressed through simple application-level preprocessing or basic routing logic. Such approaches would require each application to independently implement complex semantic analysis, maintain multiple vendor connections, handle failover logic, and manage token-based cost optimization—creating substantial duplication of effort and increasing the risk of inconsistencies across the organization. Moreover, application-level solutions would lack the centralized visibility needed for effective cost management and compliance monitoring.

This is where AI gateways come in. They are specialized tools that sit between applications and AI services and provide comprehensive solutions for these challenges. An AI gateway serves as a control plane for managing AI operations, providing organization-wide semantic request routing, intelligent caching, cost optimization, and AI provider failover capabilities. By handling these concerns at the infrastructure level rather than the application level, AI gateways enable organizations to implement consistent policies and optimizations across all their AI workloads, while reducing complexity for application developers.

## Barriers to Seamless AI Integration
While organizations understand AI’s transformative potential, several fundamental barriers can impede successful integration into existing enterprise environments.

Most enterprises operate complex landscapes of legacy systems that weren’t designed with AI in mind. These systems often use outdated data formats, incompatible APIs, and rigid architectures, with data siloed across disparate sources—from isolated relational databases and cloud-based document stores like SharePoint to flat files scattered across file servers and employee workstations. Some information may also exist only in physical formats like paper records or legacy offline media. This fragmented data landscape, combined with rigid architectures, makes AI integration particularly challenging. Real-time data access, essential for many AI applications, becomes especially problematic when dealing with such distributed and heterogeneous legacy infrastructure.

Beyond technical challenges, organizations face significant operational hurdles. The shift to AI-powered systems requires new skills, roles, and processes. Teams need to develop expertise in prompt engineering, model selection, and AI operations. Overcoming resistance to AI adoption often requires substantial training investment and cultural change.

AI systems are only as good as their data inputs. Organizations struggle with inconsistent data formats, outdated information across systems, and real-time synchronization challenges. Without proper data standardization and cleaning processes, AI implementations may deliver unreliable or inconsistent results.

Integrating AI capabilities across an enterprise requires careful architectural consideration. Organizations must manage multiple AI providers, handle different response formats, implement robust error handling, and ensure system reliability. This complexity increases with each new AI integration point, making centralized management and monitoring essential.

These barriers highlight the need for a solution that can abstract away complexity while providing the necessary controls and optimizations for enterprise AI deployment.

## Overview of the AI Gateway Solution
An AI gateway is a specialized API gateway that can semantically understand requests and responses to handle and manage AI interactions. This understanding enables capabilities that traditional networking solutions cannot provide. It serves as an intelligent intermediary for AI traffic, providing sophisticated management capabilities that enhance applications’ interactions with AI services. This intelligence is crucial for several reasons.

First, the gateway provides sophisticated credentials and data management that traditional networking components like firewalls cannot deliver. It securely handles API keys, prevents sensitive data exposure, and ensures compliance with regulatory requirements—challenges that become increasingly complex as organizations scale their AI usage. Moreover, the AI gateway makes the processing of these concerns consistent across all infrastructure, including across multiple LLM instances.

Second, it implements AI-specific performance optimizations. The gateway can intelligently load balance requests based on their semantic content, cache similar queries to reduce costs and latency, and manage sophisticated failover between different AI providers when needed.

Third, it provides comprehensive visibility and control over LLM operations. Organizations can monitor usage patterns, implement cost controls based on token consumption, and enforce guardrails that prevent harmful or noncompliant LLM interactions. These capabilities prove essential as organizations scale from initial AI experiments to production deployments.

As we’ll examine in the following chapters, traditional networking approaches fail to handle these unique aspects of AI systems. Understanding these limitations—and how an AI gateway addresses them—is crucial for organizations looking to build robust, secure, and scalable AI infrastructure. We’ll explore the specific challenges around security and compliance, resilience, and operational control, then dive into the architectural patterns and best practices for implementing an AI gateway in your environment.

# Chapter 2. The AI Gateway: Bridging the Gaps

As organizations embrace AI and LLMs, architects and IT leaders must prioritize the safe and secure adoption of these transformative technologies. In this chapter, we’ll explore the new challenges that arise when applications integrate LLMs and why traditional approaches—like conventional networking—must evolve to incorporate specialized tools tailored to this new paradigm. We will take a look at using a smart intermediary, an AI gateway, and how it improves security, observability, and performance for AI/LLM interaction, which complement traditional networking.

Why Traditional Networking Falls Short for AI Systems
To understand where traditional networking falls short, let’s take a look at a typical LLM interaction. LLMs are typically exposed through an API interface. Many LLMs have standardized on the OpenAI REST API but there are other options as well (e.g., Amazon Bedrock and Google Vertex). To make a call to an LLM, a client will prepare an HTTP request with the appropriate API keys in the headers and an appropriate prompt message for the LLM and send it over the network. For example, a client calling OpenAI from the command line would prepare a request like this:

curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
In a typical enterprise system, this HTTP call flows through the network as shown in Figure 2-1. The packets that make up the HTTP request will flow through routers to a firewall that is responsible for enforcing policy such as “is this packet allowed to go to an IP address or network?” From there it will be routed to the internet through an L3/L4 internet gateway. The OpenAI service will receive the request, process it, and return a result, which will flow back through the enterprise network and eventually back to the client.


Figure 2-1. Requests flow from a client to the LLM through a typical enterprise network that includes firewalls and routers
You may be asking, “If this is a simple HTTP request, what’s so special about LLMs?” Great question. This simple LLM API call brings up a number of scenarios that are similar to existing HTTP requests, but also introduces some differences that can cause issues that require specialized API tooling. Let’s dig into what those issues are.

Security Practices
The first thing you may notice is that the call to the LLM requires an API key that gets sent in one of the headers. This API key is sensitive and should only be sent on HTTPS requests so it’s encrypted on the wire. This happens transparently when calling the OpenAI API for (i.e., by calling https://api.openai.com). But what about storing and retrieving the key to be used in the request? How will developers keep the key safe? If the key is compromised, whether through some nefarious act or by sharing the key with other teams, it will become a security issue.

Unfortunately, with more scale, this means many sensitive API keys scattered around a production environment and higher chances for these keys to get compromised. Not all teams will correctly follow secure practices all of the time. For example, a common scenario is for developers to accidentally hardcode API keys directly into their code and then check that code into a repository. Another practice, unfortunately common, is to see API keys specified in environment-specific configuration files. These key leaks can be very expensive.

An AI gateway can complement traditional networking security to solve these challenges. We’ll cover how to do this in Chapter 3.

Resilience
Latency and load balancing pose significant challenges when integrating LLMs into enterprise workflows. Traditional load-balancing methods, such as round-robin or connection-based strategies, are too simplistic because they don’t take into account the semantic meanings of the prompts or help decide which model is best suited to service a request. Additionally, without semantic understanding, there is no effective way to cache responses for repeated queries, which could significantly reduce latency and server load. Enterprises need intelligent, context-aware systems that can cache similar or identical semantic queries and distribute workloads more effectively to minimize response times and improve system performance.

Adding to the complexity, enterprises often need the flexibility to switch between model providers—such as OpenAI, Anthropic, or custom domain-specific models—based on strategic motivations, cost, or specialized use cases. This requires infrastructure capable of dynamically rerouting workloads while maintaining performance, minimizing downtime, and ensuring seamless transitions across models, whether they’re general purpose or tailored for specific needs.

Another concern is model failover and reliability. AI models may experience performance degradation due to model-specific issues, such as high latency during peak usage or outdated training data that fails to deliver accurate responses. Ensuring robust failover mechanisms for these models is essential. For instance, enterprises need strategies to redirect traffic seamlessly to backup models or alternate providers when the primary model isn’t performing adequately. This type of failover is more complex than standard systems, as it must account for differences in model output quality and cost while maintaining a consistent user experience.

Cost Control and Observability
Controlling costs is a critical consideration for organizations integrating LLMs into their workflows. The dynamic nature of LLM usage often makes it difficult to track who is calling these services, under which budget, and how much is being spent. Without proper visibility, enterprises risk spiraling costs as usage grows unpredictably across teams and projects. Implementing mechanisms to monitor and attribute LLM usage to specific teams or applications can provide clarity. Furthermore, setting rate limits based on token consumption (tokens are an LLM-specific concept) or enterprise-established quotas ensures that resources are used efficiently and within budget. These limits are also essential to prevent excessive usage that could lead to denial-of-service (DoS) conditions or resource starvation for critical operations.

Observability is another crucial aspect of managing LLM interactions. Enterprises must monitor key networking metrics such as error rates and latency to identify performance bottlenecks or systemic issues in their LLM workflows. However, LLMs introduce additional, model-specific concerns that go beyond traditional observability practices. For example, tracking prompt evolution (are prompts excessively long? is the structure changing to deliver sub-optimal results?), token latency (TTFT, time per output token, etc.), context-window usage, and usage attribution are all concerns.

Guardrails and Compliance
Application developers may feel free to call an LLM to implement some part of their applications, but how often do they consider the sensitivity of what is being sent in those prompts? In an enterprise setting, sending sensitive data is not just a simple oversight; it’s a major compliance risk. Enterprises operate under strict data protection and confidentiality requirements. You can’t just send data flying around without adhering to those constraints. The same is true when working with LLMs. Sensitive data—whether it’s PII, health-related under HIPAA, or subject to regional protections like GDPR—must be handled with the utmost care to meet regulatory requirements. In fact, some of the most important data, which may not be covered under compliance oversight, but is extremely sensitive, is a company’s proprietary data and trade secrets. Not all teams will have to deal with regulated data like PII, but most teams will deal with sensitive company-specific data.

This is where guardrails come into play. Think of them as the WAF for your LLM queries—enforcing rules about what’s acceptable, validating inputs, and constraining outputs. These guardrails are crucial as you scale your AI operations. For instance, they can block prompts designed to elicit biased or harmful responses, or that risk exposing sensitive information. Now, you might wonder: can’t developers just bake these protections into the app code? Sure, but that approach is risky. Just like we don’t leave the entire burden of cybersecurity to developers, the same principle applies here. A “defense-in-depth” strategy is essential. Enterprises succeed in cybersecurity by layering robust, externalized controls over application-level protections, and the same approach applies to LLMs. A well-designed governance layer that combines cost control, observability, and intelligent guardrails isn’t just a nice-to-have—it’s the key to scaling AI responsibly, predictably, and confidently.

Tackling these LLM-specific challenges like resilience, guardrails, and security will require tooling that is specialized and can take these challenges into account. Traditional networking can be augmented with specific tools like an AI gateway to solve these challenges. In the next section, we look at how to approach an AI gateway.

The Role of an AI Gateway in Secure and Scalable AI Adoption
An AI gateway is a specialized API gateway that can semantically understand the details of a request to better implement security, guardrails, observability, failover, and load balancing. You can use an AI gateway in conjunction with your existing API gateway or as a modern option to replace last-generation API management for cloud-first, platform-based, self-service deployments. An AI gateway is typically used for requests going out to an LLM that’s a publicly hosted service. As you can see in Figure 2-2, calls go from a client to an AI gateway and then out to the LLM (OpenAI, in this case). In this scenario, the AI gateway acts as a powerful forwarding proxy.

An AI gateway plays a pivotal role in helping enterprises adopt and scale AI/LLM usage securely and efficiently. Many organizations begin their AI journey by using public models like OpenAI for their flexibility and ease of integration. However, as adoption grows, they often transition to private or hybrid setups to address concerns regarding cost, data privacy, and compliance.


Figure 2-2. Requests to the LLM follow a forwarding proxy model
Security and privacy are foundational aspects of an AI gateway. Organizations must ensure that sensitive enterprise data is not inadvertently exposed to public LLMs. Customizable guardrails enable masking, blocking, or filtering of sensitive data in requests and responses, protecting data at every stage of the interaction. Additionally, centralized credential management secures API keys using secret stores and enforces fine-grained access with role-based access control (RBAC). Combined with robust authentication policies like JSON Web Tokens (JWTs) and external authentication mechanisms, the gateway ensures only authorized users can access AI APIs.

AI gateways enable operational visibility. Detailed metrics, such as request volume, token usage, and latency, provide insights for managing performance and cost. Robust logging and monitoring facilitate compliance reporting, troubleshooting, and auditing, ensuring AI adoption aligns with regulatory requirements. Integrating with existing observability tooling through open standards like OpenTelemetry helps augment existing initiatives to build smart dashboards for end users.

Since AI gateways utilize semantic understanding of requests/prompts, they can make smarter decisions about load balancing, failover, or even caching of the results. Semantic awareness enables the gateway to attach additional context to the prompt following organization-approved methods. For example, an AI gateway can help implement retrieval-augmented generation (RAG), a mechanism to add additional facts or organization-specific data to a prompt to guide the LLM to more accurate responses.

Designing and Deploying an AI Gateway: Best Practices
Many organizations are moving to highly automated, self-service workflows for their developers based around an internal developer platform, and an AI gateway must fit in with this platform. We’ll cover more about this in the last section of this chapter, but to facilitate this integration, we must take the following into account when considering AI gateway design:

Separation of concerns

Powerful foundational proxy

Declarative configuration

Open source

Separation of Concerns
Designing an AI gateway starts with clear separation of concerns. That is, the gateway should act as an operational layer, distinct from your application’s business logic. By decoupling these concerns, you can focus on enhancing the application’s functionality without constantly modifying operational code. For example, masking sensitive data, validating inputs, or enforcing API quotas should be implemented as part of the gateway, not embedded within the application itself.

Leverage a Powerful and Flexible Proxy
The choice of a proxy to serve as the foundation of your AI gateway is critical. The proxy should be performant, configurable, and mature. Envoy is known for its performance, scalability, and extensibility. Envoy’s powerful filter chain architecture enables flexibility, with standard plug-ins for features like rate limiting, external authentication (ext-auth), and general external processing (ext-proc). These capabilities enable the gateway to handle complex tasks, such as manipulating request and response bodies to mask sensitive data, without overburdening application code. With Envoy, you can rapidly develop and deploy advanced gateway features while ensuring the system remains performant and scalable.

Declarative Configuration
Declarative configuration is a best practice for managing an AI gateway, ensuring predictability and reducing the risk of human error. Using a declarative approach allows you to define the desired state of your gateway’s behavior, such as routing rules, security policies, and API quotas, in a structured format like YAML or JSON. This makes it easier to version control configurations, apply them consistently across environments, and automate updates. Additionally, declarative configurations integrate seamlessly with infrastructure as code (IaC) practices, promoting a unified and repeatable deployment process.

Roots in Open Source
Building your AI gateway on open source foundations provides several advantages. Open source technologies like Envoy come with large, active communities that contribute to ongoing innovation, security updates, and feature development. This reduces vendor lock-in and enables enterprises to stay ahead of evolving technical requirements. Leveraging open source software aligns with modern development practices, offering cost-effectiveness and flexibility in adapting the gateway to meet specific enterprise needs.

AI Success Requires Platform Engineering
IT organizations often struggle to get their multiple teams on the same page. Teams often make decisions in silos, leading to duplication of tools, integration challenges, and breakdowns in governance, which can result in compliance issues. Platform engineering plays a pivotal role in uniting diverse IT teams to streamline software delivery and operational excellence. By fostering collaboration among application developers, security specialists, networking experts, and others, platform engineering initiatives aim to reduce friction, enhance developer productivity, and enforce compliance standards.

AI gateways designed with automation-first principles, such as GitOps and IaC, can plug into these environments with minimal disruption. They enable enterprises to supercharge their platform efforts, embedding AI-powered insights and orchestration into the workflows that teams already use. For example, data scientists and developers can collaborate more effectively, using the AI gateway to enable self-service access to models, APIs, and infrastructure while maintaining robust security and governance. This approach not only accelerates AI adoption at scale but also reinforces the platform engineering mission of empowering teams with self-service tools and ensuring alignment across the organization.

As organizations introduce AI and LLM usage, an AI gateway becomes crucial. It acts as a bridge, weaving advanced semantic understanding into workflows to mitigate the risks of escalating costs, data privacy concerns, and compliance complexities—key issues in enterprise environments. An AI gateway designed with the previously discussed best practices (separation of concerns, powerful proxy, declarative configuration, and open source) is designed to integrate nicely into internal developer portals. In the next chapter, we look at an AI gateway implementation that was built with these best practices.

# Chapter 3. Common AI Gateway Use Cases
In the previous chapter, we introduced the concept of an AI gateway and explained why traditional networking falls short. This chapter will explore how enterprises can use an AI gateway to solve real-world challenges.

As we explore these use cases, we’ll reference an AI gateway implementation as an example. Users should choose an AI gateway built on modern architectures including Envoy Proxy and Kubernetes Gateway API. Gateways like Gloo AI Gateway (being donated to Cloud Native Computing Foundation—CNCF—as KGateway) enable AI features that accelerate AI application development while addressing critical security, observability, control, and governance needs.

Choosing a reliable AI gateway helps address the top concerns that enterprises have when adopting AI, including:

Security threat mitigation

The bridging of the technical skills gap

Seamless integration with existing infrastructure

With this context in mind, let’s explore the key use cases that make AI gateways essential for enterprise AI adoption.

Security and Access Control
Security and access control are paramount for every technology, and AI use cases aren’t any different. An AI gateway can encompass everything from managing API credentials to multi-tenant isolation.

In this section, we’ll explore how an AI gateway provides a security layer that handles authentication, authorization, and access policies to ensure that AI resources and LLM providers are accessed only by authorized users and applications.

API Key and Credential Management
Because every LLM provider requires an API key, managing them presents a significant security challenge in enterprise environments. The traditional approach of embedding API keys directly in application code or configuration files leads to credential sprawl, increasing the risk of accidental exposure through code repositories or configuration leaks. As organizations scale their AI adoption and integrate multiple LLM providers, this problem is compounded.

Figure 3-1 shows how an AI gateway addresses these challenges by centralizing credential management at the infrastructure level, away from the application code. This allows operators to securely store and manage API keys for multiple providers while integrating with enterprise secret management systems, as shown in Figure 3-2.


Figure 3-1. Comparison of direct versus gateway-mediated access to LLMs

Figure 3-2. Integration of enterprise secrets management system with an AI gateway architecture
By removing credential management responsibilities from application teams, organizations can implement consistent security practices, automate key rotation, and maintain a comprehensive audit trail of credential usage across all AI workloads.

Fine-Grained Access Control and Auditing
While centralizing LLM credential management solves one part of the security equation, organizations also need to control access to LLMs. Users should select an AI gateway that can integrate with existing enterprise authentication and authorization mechanisms. Whether an organization uses JWTs, custom tokens, OpenID Connect (OIDC), Lightweight Directory Access Protocol (LDAP), or other authentication methods, AI gateways can leverage these existing systems rather than introducing new credentials to manage.

Let’s look at an example using JWTs, though similar principles apply to other authentication methods. JWTs serve as one approach for implementing fine-grained access control in AI gateways.

Unlike API keys, JWTs carry rich claims (key-value pairs) about the user’s identity, roles, permissions, and any other values. This enables fine-grained and flexible access control decisions.

Here’s an example of a JWT with custom claims:

{
  "iss": "sso.solo.io",
  "sub": "peterj",
  "team": "ai-research",
  "llms": {
    "openai": ["gpt-4", "gpt-3.5-turbo"],
    "anthropic": ["claude-3-opus"]
  }
}
Based on the claims in the JWT, the AI gateway can enforce access control at multiple levels. Let’s look at a couple of examples.

Team and LLM access control
This example restricts which models can be accessed or enforces team isolation (that is, restricts access by team) based on the values in the JWT claims using RBAC, as shown in Figure 3-3. The example only allows access to the openai-route if JWT contains the team: ai-research value:

apiVersion: gateway.solo.io/v1
kind: RouteOption
metadata:
  name: jwt-ai-research
spec:
  targetRefs:
  - group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: openai-route
  options:
    rbac:
      policies:
        viewer:
          nestedClaimDelimiter: .
          principals:
          - jwtPrincipal:
              claims:
                team: ai-research

Figure 3-3. Using RBAC to allow access to the AI research team and restricting access for the QA team through an AI gateway
Similarly, we could come up with a configuration that restricts which models a user can access based on the JWT claims.

Usage control
The gateway operators can use a declarative configuration to enforce token rate limits based on claims in the JWT as shown in Figure 3-4. The example below configures a rate limit of 500 tokens per hour for each JWT subject (sub) claim. This rate limit configuration is also separately applied to a specific route on which the AI gateway enforces it:

...
    descriptors:
    - key: user-id
      rateLimit:
        requestsPerUnit: 500
        unit: HOUR
    rateLimits:
    - actions:
      - metadata:
          descriptorKey: user-id
          source: DYNAMIC
          default: unknown
          metadataKey:
            key: "envoy.filters.http.jwt_authn"
            path:
            - key: principal
            - key: sub

Figure 3-4. Token rate limiting with per-subject quota from JWT
Securing Multi-Tenancy in AI Workflows
Large enterprises typically have multiple business units, development teams, or external partners that need access to AI capabilities. Consider a financial services company where different departments—wealth management, retail banking, insurance—all need to use LLMs for various purposes. Each department has specific security requirements, data handling needs, and budget constraints.

By combining an AI gateway and a solution such as a service mesh, enterprise teams can enable secure multi-tenancy. This allows organizations to isolate each tenant while maintaining control and governance.

For example, the wealth management division might need to use OpenAI’s GPT-4o model for analyzing investment reports, while the retail banking team uses a different model or provider for customer service automation.

With this approach, you can ensure the following for each tenant:

Separate API credentials and rate limits

Division-specific prompt templates and configurations

Isolated data handling policies

Individual cost centers and usage quotas

Custom security policies and compliance rules

Data Privacy and Compliance Enforcement (Guardrails)
Modern enterprises navigate large sets of private and sensitive data on a day-to-day basis, requiring strict data compliance and confidentiality provisions. Additionally, all sensitive data in enterprises must follow regulatory and compliance requirements, including HIPAA, GDPR, and other applicable regional requirements such as the California Consumer Privacy Act (CCPA).

An AI gateway serves as an intermediary that inspects, filters, and governs all data flowing between all applications and AI models and backends. In this section, we’ll explore how enterprises can implement data protection strategies, including content filtering and PII detection.

To protect data, enterprises must implement guardrails and data loss prevention (DLP) strategies. The guardrails in the AI gateway provide bidirectional protection for both incoming requests (prompts or queries from the end users or applications) and outgoing responses from the LLMs.

The protection can be done by simply rejecting responses and returning a predefined message (such as, “Rejected due to inappropriate content”) or by masking them. Masking involves replacing the sensitive content with a specific word or character. For example, if we want to mask locations, the string “Italy is a great place to visit” might look like this: “[LOCATION] is a great place to visit.” Another option is to completely mask the detected string without giving a clue as to what it might be: “XXXXX is a great place to visit.”

For incoming requests on the gateway, the guardrails kick in before the request is sent to the LLM. This allows detection and automatic blocking of requests before they reach the LLM, ensuring compliance with data privacy regulations. This approach prevents any sensitive data from being inadvertently shared with external AI services.

On the response side, the LLM outputs may be enriched with enterprise data that contains sensitive information. While useful as a context and to provide the most accurate information, it’s not necessarily desirable to return the sensitive data to the caller. The guardrails on the response can detect PII and apply masking rules or, when necessary, reject the response entirely and replace it with a predefined message such as “Rejected.” This ensures that protected information remains secure even when LLMs incorporate internal data sources. Figure 3-5 shows the flow of unsanitized and sanitized inputs and outputs with input and output guardrails.


Figure 3-5. Content sanitization workflow with input and output guardrails within an AI gateway
A guardrails system—for example, Microsoft Presidio—typically employs one or more of the following methods to evaluate and filter content in both directions:

Built-in patterns
Can be used to detect credit card numbers, social security numbers, emails, and phone numbers.

Regular expressions
Can be used to reject or mask the matched strings.

External moderation
Can be used to determine whether the request/response should be masked or rejected. This may be an external model such as omni-moderation from OpenAI or a custom webhook.

Beyond direct content filtering, organizations can further enhance privacy and compliance through prompt templates. These templates allow system prompts or additional context to be automatically attached to user queries, establishing consistent ground rules for all LLM interactions.

For example, in Figure 3-6 we automatically attach a system prompt instructing the LLM to perform sentiment analysis on the user’s query and respond with a single word in French. With the ability to do this through the AI gateway, organizations can define and enforce standardized system prompts and automatically apply them to all client requests, ensuring adherence to privacy policies and compliance requirements without requiring individual applications to specify these rules in each query. This templating approach creates a unified governance layer that standardizes how applications interact with LLMs while maintaining security protocols.


Figure 3-6. Composing user input and predefined system template allowing standardized LLM interaction
Performance and Reliability
AI workloads present unique challenges in terms of performance and reliability management. From handling varying response times to managing high-concurrency scenarios, AI gateways must ensure consistent and reliable service delivery.

Performance Metrics and User Experience
When managing AI workloads through an AI gateway, understanding and optimizing key performance metrics is crucial for service reliability. The primary metrics that impact user experience are latency-based, particularly in streaming scenarios where responses are generated token by token. By default, the responses from the LLM providers are returned once the complete response is generated.

Two critical metrics for monitoring gateway performance are TTFT (which we discussed in Chapter 1) and time per output token (TPOT).

TTFT measures the initial response time from query submission to the first generated token. This metric is important for real-time applications like chatbots, where users expect immediate responses. The AI gateway’s traffic management decisions can significantly impact TTFT by optimizing routing and load distribution.

TPOT measures the generation speed of subsequent tokens. For streaming responses, TPOT should align with human reading speed (approximately 5-6 tokens per second, depending on the complexity of the text and reading speed) to ensure a smooth user experience. The AI gateway can optimize TPOT through intelligent load balancing and resource allocation.

To calculate the total latency, take the TTFT and add it to the product of the TPOT and the number of output tokens, as shown in Figure 3-7.

When estimating expected latency, be mindful that response times can be delayed if the AI agent you’re interacting with is under a heavy workload and performing multiple LLM requests as part of the same interaction. For that reason, it makes sense for your performance tests to generate a workload heavy enough to reflect expected production conditions.


Figure 3-7. Latency components in LLM response generation
Traffic Management and Scaling for AI Workloads
Robust traffic management is crucial for maintaining reliable and cost-effective services. An AI gateway needs to efficiently route requests, balance loads across endpoints, and enforce policies while ensuring optimal performance and resource utilization.

For example, Gloo AI Gateway—also known as KGateway—leverages Envoy proxy’s traffic management capabilities to handle the AI workloads running across multiple teams and accessing different providers.

In this scenario, Gloo AI Gateway implements both distributed and global load-balancing strategies to optimize traffic flow. It uses active health checking and intelligent load-balancing algorithms to route requests across model endpoints based on their current performance, availability, and API quota. At the global level, the gateway will enforce organization-wide traffic policies, such as preferring certain model providers or endpoints based on cost, performance, or geographical location.

The routing and traffic management are based on various request attributes. For example, organizations can configure routing rules that direct requests to different model endpoints based on:

Request headers that indicate the complexity or priority of the query

Path parameters that specify model capabilities or versions

Team or application identifiers that determine routing preferences

Resource utilization patterns that optimize for cost and performance

Provider token usage limits (i.e., route to different models when token limits are reached)

Health checking and outlier detection mechanisms continuously monitor endpoint health, automatically removing degraded instances from the load-balancing pool. In case of failures, the AI gateway can automatically failover and switch to a backup system.

In this example, Gloo AI Gateway can also handle failover for the models of the LLM providers that you want to prioritize. If the main model from one provider is unavailable, becomes slow, or has any issues, the AI gateway can quickly switch to a backup model from the same or different provider. This keeps your services running without interruption.

Integration of Hybrid AI Solutions
While public, cloud-based LLMs are probably the easiest way to start working on a proof of concept, organizations often need to leverage multiple models, both public and private, deployed across different infrastructures. This hybrid approach enables enterprises to balance the advanced capabilities of various public models with the security and control offered by different private model deployments. An AI gateway is in the perfect position to serve as an orchestration layer for managing this architecture and providing unified routing, security, and governance across all types of models.

For example, an enterprise might use a public model like GPT-4o for general-purpose tasks, while routing sensitive queries involving private data to custom or fine-tuned models running on its own infrastructure. The AI gateway can make these routing decisions based on factors including the content type, security requirements, or specific headers in the request. This intelligent routing ensures that each query is directed to the most appropriate model while maintaining consistent security policies and monitoring across the entire infrastructure and AI resources.

Optimization and Monitoring
As AI usage grows within an organization, understanding usage patterns, controlling costs, and maintaining visibility into system performance becomes increasingly important.

In this section, we’ll explore how AI gateways can help enterprises optimize their AI investments through features like semantic caching and intelligent routing, while providing the observability needed to maintain and improve system performance.

Cost Optimization and Budget Controls
Semantic caching is an approach to reducing computational costs and improving response times when working with LLMs. Unlike traditional caching that relies on exact matches, semantic caching understands the intent behind queries, allowing for the reuse of previous responses even when questions are phrased differently.

Since every response generated by an LLM incurs some cost, every positive hit in the semantic cache means fewer cost-incurring requests made to the LLM provider. Semantic caching also improves response times when compared to responses generated by an LLM, typically measured in seconds. Better response times mean better user experience.

Let’s take a look at an example. Consider a customer service AI that handles product inquiries. If one customer asks “What’s the return policy for electronics?” and another later asks “How many days do I have to return my laptop?”, traditional caching would treat these as different queries requiring separate calls to the LLM. However, semantic caching recognizes their similar intent and can serve the cached response from the first query to the second customer, saving both time and cost, as shown in Figure 3-8.


Figure 3-8. Using a vector database to retrieve semantically similar requests
It’s important to note that implementing semantic caching itself requires embedding models or smaller LLMs at the gateway level to analyze query intent, which introduces its own computational overhead and complexity. This means organizations need to carefully evaluate the trade-off between gateway costs and potential savings from reduced LLM calls.

The cost savings of semantic caching can be substantial. If we continue with the previous example, let’s say there are 100,000 customer queries per month using GPT-4, with an average cost of $0.03 per query. Without semantic caching, this would cost $3,000 a month. If semantic caching can handle 60% of queries (a conservative estimate for common customer service scenarios), the monthly cost drops to $1,200—a savings of $1,800. Over the course of a year, this represents $21,600 in direct LLM cost reductions, not including the additional savings from reduced computational resources and improved response times.

Another optimization that impacts latency, and improves perceived responsiveness and TTFT metrics is streaming mode. In streaming mode, the LLM sends tokens to the client as they are generated, rather than waiting for the entire response to be completed, allowing users to see the response build progressively in real time. Streaming mode is provided by the LLM backend and must be implemented and enabled in the AI gateway.

Monitoring and Observability for AI Pipelines
Just like for any application, good monitoring and observability are crucial when running applications that use AI and LLMs. The operating unit of measurement and cost is tokens. They are the units that organizations are charged for. Both input and output tokens matter: the input tokens are the prompts and queries that are sent to the LLM and the output tokens represent the response from the LLM. The costs for both are typically represented per one million tokens and differ by LLM provider.

Depending on the AI provider and the model you’re using, you should expect the cost to be anywhere from a few cents to a few dollars per million input tokens, while the output token responses will typically be a few times more expensive.

With varying costs for tokens, it’s clear why comprehensive monitoring is important. An efficient observability system enables organizations to identify issues early, allowing them to optimize model usage, track costs, and ensure that budgets are not exceeded.

In “Performance Metrics and User Experience”, we talked about the performance metrics TTFT and TPOT. Here are some other metrics, grouped into categories, which can provide valuable insights into your AI applications:

Resource utilization

Token consumption patterns by model and endpoint

Request distribution across different LLM providers

Semantic cache hit rates and efficiency metrics

Concurrent request patterns and queue depths

Quality metrics

Completion success rates

Error rates and types (context length, content filtering, timeout)

Response coherence scores

Hallucination detection rates

Financial metrics

Cost per request by model

Token utilization efficiency

Budget adherence by team/project

Cost anomaly detection

User experience

TTFT

TPOT

TPS

Total request latency distribution

Conclusion
AI gateways serve as a critical foundation for enterprise AI adoption by providing comprehensive solutions for security, performance optimization, and observability challenges. Features like centralized credential management, semantic caching, and detailed monitoring capabilities enable organizations to safely scale their AI implementations while maintaining control over costs, security, and compliance requirements.

In the next chapter, we’ll look ahead at how these foundational capabilities will evolve in the future.

# Chapter 4. The Future of Enterprise AI
AI makes for an exciting and innovative period of time in information technology. While many organizations are leveraging AI to either optimize and make business workflows more efficient, or to innovate and provide better user experiences, we believe more and more use cases will take advantage of this technology. Scaling AI use cases and adoption will require specialized tooling that is “AI-aware” such as an AI gateway as discussed in this report. Let’s see how the future of AI in enterprises contributes to the challenges of scale, security, observability, and compliance.

Improvement of Models, Continued Adoption
We expect that enterprises will continue to innovate and invest in AI and LLM-powered use cases. LLMs are helping to automate routine tasks, such as system monitoring, incident response, and infrastructure optimization, enabling faster resolution times and improved efficiency. For security and threat detection, these models enhance the ability to identify patterns, detect anomalies, and respond to cyber threats in real time. Customer service and support are also undergoing a shift, with AI-powered chatbots and virtual assistants delivering personalized, around-the-clock support that improves customer satisfaction and reduces operational costs. These investments are driving strategic business outcomes and helping businesses maintain a competitive advantage.

Alongside the continued expansion of AI-backed use cases, we expect the underlying models to become more powerful and cheaper. Over the next year, we can expect significant advancements in LLMs across several key areas. Context windows are expanding dramatically, with some models like Anthropic’s Claude already reaching 100,000 tokens, and Google’s Gemini supporting up to two million tokens. These numbers may be outdated by the time you read this, but the fact remains that these models will continue to extend context capabilities, which will allow models to analyze even larger amounts of content and datasets in a single interaction. Reasoning capabilities are also improving rapidly, as seen with OpenAI’s o1 and o3 models, which emphasize enhanced logical consistency, multistep problem solving, and contextual understanding. Lastly, the cost to train these models will likely drop as seen with the claims of the recent release of DeepSeek-R1. This progression will make AI more adept at addressing complex, nuanced queries across enterprise domains.

Adopting Internal Models
Enterprises are increasingly exploring the potential of running and training their own language models, leveraging small language models (SLMs) or open source options like Llama, Mistral, and others. This trend is driven by the need for greater control over data privacy, cost efficiency, and customization. SLMs and open source models enable organizations to tailor models to their specific domain requirements while reducing dependency on third-party vendors. These smaller, fine-tuned models can deliver competitive performance for many use cases, such as document summarization, customer support, and predictive analytics, without requiring the massive infrastructure and resources of larger LLMs.

Kubernetes plays a critical role in operationalizing these AI workloads, as it is already the strategic runtime container platform for most enterprises. Kubernetes provides the scalability, resource management, and orchestration necessary for running inference workloads efficiently across distributed environments. By containerizing model training and inference tasks, organizations can integrate AI into their existing continuous integration, continuous delivery (CI/CD) pipelines and manage workloads at scale. Moreover, tools like KServe enable the automation of model training, deployment, and monitoring, allowing teams to adopt AI in a cost-effective and production-ready manner. This synergy between Kubernetes and enterprise AI adoption aligns nicely with platform engineering efforts currently underway at most organizations to provide a consistent user experience, self-service, and improved compliance.

Better Control over Model Inference with Extensions to Gateway API
The Kubernetes community is actively advancing its support for inference workloads through initiatives like the LLM Instance Gateway proposal, developed by the WG Serving working group and sponsored by SIG Apps. Inference workloads in this context means running models that have already been trained on Kubernetes, usually utilizing some specialized hardware like GPUs. This proposal introduces specialized load-balancing algorithms, custom resource definitions (CRDs), and controllers tailored to calling LLM services, enabling efficient routing and resource management for requests going to running models, while taking into account specialized accelerators and fine-tuned models. By multiplexing LLM services over shared compute pools, the initiative aims to optimize resource utilization and improve inference performance, addressing the unique challenges of deploying AI workloads in Kubernetes environments. These efforts align with Kubernetes’ role as the strategic container runtime for modern applications, further streamlining the integration of cutting-edge AI capabilities into enterprise infrastructure while maintaining scalability, efficiency, and ease of operation.

Autonomy and Agentic Workflows
The future of AI is focused on transforming LLMs from reactive chatbots into proactive, autonomous agents capable of sophisticated multistep planning and execution. Agentic AI systems—those that can act independently and make decisions on behalf of users—represent a significant evolution beyond current capabilities. Unlike traditional LLMs, which respond passively to user prompts, these next-generation AI agents will operate with a degree of autonomy, dynamically analyzing goals, prioritizing tasks, and executing them across various domains without constant human oversight. For example, instead of simply answering a query about generating a report, an AI agent could autonomously collect relevant data, synthesize insights, format the document, and even schedule a presentation for stakeholders.

These advancements hinge on integrating LLMs with reinforcement learning, planning algorithms, and external systems. By leveraging tools and APIs, autonomous AI agents will interact seamlessly with other software, databases, and Internet of Things (IoT) devices, enabling them to perform actions like automating business workflows, managing cloud infrastructure, or even running complex simulations. Frameworks such as CrewAI or LangChain exemplify early steps in this direction, allowing AI agents to decompose high-level goals into actionable subtasks and execute them iteratively. As these agents become more sophisticated, they could revolutionize industries ranging from customer service to robotics, ushering in a new era of proactive, goal-driven AI systems that extend far beyond traditional conversational capabilities.

Next Steps
As enterprises continue to integrate AI into their operations, the complexities of managing AI workloads—particularly in areas like security, observability, compliance, and scalability—are becoming increasingly evident. To address these challenges, tools like Gloo AI Gateway (KGateway) and Portkey AI Gateway have emerged as essential components in the enterprise AI ecosystem. By implementing an AI gateway, enterprises can establish a robust framework that not only accelerates AI innovation, but also ensures that AI applications are deployed securely, efficiently, and at scale. This positions organizations to effectively harness the transformative potential of AI while maintaining control over operational complexities.

About the Authors
Christian Posta (@christianposta) is VP, Global Field CTO at Solo.io. He is the author of Istio in Action (Manning), as well as many other books on cloud native architecture, and is well known in the cloud native community for being a speaker, blogger, and contributor to various open source projects in the service mesh and cloud native ecosystem (Istio, Kubernetes, et al.). Christian has spent time at government and commercial enterprises, as well as web-scale companies, and now helps organizations create and deploy large-scale, cloud native, resilient, distributed architectures. He enjoys mentoring, training, and leading teams to be successful with distributed systems concepts, microservices, DevOps, and cloud native application design.

Peter Jausovec is a senior principal technical marketing manager at Solo.io with over 17 years of experience spanning software development, QA, and engineering leadership. A recognized expert in cloud native technologies, he specializes in Kubernetes and Istio. Peter authored the book Cloud Native (O’Reilly) and pioneered several industry-leading initiatives, including developing comprehensive Envoy proxy and Istio training courses that have educated over 15,000 learners. He created the first-ever Istio certification program, now the Istio Certified Associate exam under the Linux Foundation. At Solo.io, Peter has been instrumental in shaping next-generation technologies, including prototyping the AI Gateway.