Chapter 1. Why AI Adoption Is Critical for Enterprise Success
In this chapter, we’ll explore why AI adoption has become critical for enterprise success and examine how organizations are leveraging AI across different business functions to drive transformation and competitive advantage.

Artificial intelligence has become a hot topic over the past couple of years. It’s transforming the enterprise technology landscape. Building on previous technological revolutions—from word processors that fundamentally changed document creation and workflow, CRM systems that transformed how organizations manage customer relationships, and accounting software that revolutionized financial operations—AI is introducing a new dimension of change. While these earlier innovations each transformed specific business functions, AI’s impact is uniquely pervasive, simultaneously enhancing customer experiences, streamlining operations, and driving innovation across virtually every department and process within organizations.

Data from recent research shows the impact of AI adoption. According to a comprehensive global survey of 2,500 senior business leaders conducted by Google Cloud, 74% of enterprises using AI in production already see a return on investment (ROI) within their first year. Moreover, 86% of organizations seeing revenue growth from AI report gains of 6% or more in annual revenue.

Similarly, a CIO Tech poll study found that 70% of enterprises will increase their AI spending over the next year in anticipation of their organizations using AI tools to transform their business in the next three to five years.

These statistics highlight that AI adoption is not just driving incremental improvements—it’s enabling business transformation across key dimensions. The stats below are all from the Google Cloud survey:

Customer experience
AI enhances how organizations engage with customers. The Google Cloud survey shows that 85% of companies improving user experience with AI report measurable increases in user engagement. From chatbots to personalized recommendations, AI is reshaping the customer journey.

Productivity
Among enterprises reporting productivity improvements from AI, 45% note a doubling—or more—of employee output. This underscores AI’s ability to automate repetitive tasks and transform how work gets done.

Business growth
AI is fueling growth on multiple fronts, with 63% of organizations reporting direct business improvements due to AI implementation:

45% of organizations see engineering and developer productivity at least double after AI deployment.

84% of organizations report accelerating technical teams and allowing them to move AI use cases to production in less than 6 months.

70% of organizations report faster time to insight and 59% improve accuracy in core IT processes and infrastructure.

Security
Of the 56% of organizations that improved their security posture with AI, 82% cite better threat detection, while 71% report reduced incident response times.

The transition from proof of concept to production system marks a turning point in AI adoption. Today, 84% of organizations can deploy AI use cases from idea to production in under six months, a remarkable acceleration compared to prior technological innovations. Unlike previous technologies that transformed specific business functions, this rapid deployment capability enables AI to drive simultaneous changes across an organization’s entire operational landscape—from infrastructure and core processes to customer-facing services and business strategies. This horizontal transformation across multiple domains represents a fundamental shift in how technology reshapes enterprises.

AI adoption is no longer optional for organizations that want to maintain competitive relevance. Early AI adopters are creating competitive advantages by building hands-on experience implementing AI solutions, developing deep AI expertise across their teams, reshaping their workflows to optimize AI integration, and creating evaluation feedback systems (evals) that continuously improve their AI models.

Implementing AI at scale presents significant technical challenges for architects and platform engineers. Organizations moving from proof of concept to production face complex requirements around security, governance, and scalability. Successfully deploying AI requires building robust technical foundations that can:

Secure sensitive data and API credentials across multiple AI providers

Manage and optimize resource consumption at scale

Ensure consistent performance and reliability

Maintain compliance and governance across the organization

Enable seamless integration with existing systems and workflows

Those who successfully address these technical challenges while balancing rapid deployment with long-term strategy will position their organizations for success in an AI-driven world.

Common AI Use Cases
Fueled by the emergence of big data architectures, AI technology has finally moved from academic research centers to the mainstream. The rise of new tools such as large language models (LLMs) is transforming everything from how teams collaborate to how businesses crunch their data.

Note
An LLM is an artificial intelligence system trained on vast amounts of text data to understand and generate human-like text. Large in LLM refers both to the size of the training data and the model’s parameters (the mathematical values that determine its behavior). An LLM works by predicting the most probable next word in a sequence based on its training data and context. Think of it like an autocomplete system that can understand context, answer questions, and write text. Key examples of LLMs include GPT-4, Claude, and Llama.

Let’s explore the major ways businesses are putting AI to work today, keeping in mind that new use cases emerge regularly. The keys to success are identifying where AI can provide the most value for your specific organization and implementing it in ways that enhance rather than replace human capabilities.

Enterprise Productivity and Knowledge Work
AI gets its power from data. Data plays a dual role in AI: it forms the foundation that LLMs are trained on and it serves as a crucial context for their operation. Financial analysts use AI to rapidly extract insights from quarterly reports and market trends, turning hours of manual analysis into minutes of automated processing. Marketing teams leverage AI to analyze campaign performance data and generate content variations. Research teams use AI to summarize academic papers and identify relevant studies across vast databases of publications.

LLMs are powerful for tasks like document analysis and summarization, report generation and data visualization, market research and trend analysis, email management and response drafting, meeting summarization and action item extraction, and more.

Customer Experience and Engagement
Customer service is one of the top areas where organizations see immediate ROI from AI implementations.

Modern AI-powered customer service systems do far more than simply answer basic questions. They understand and process natural language to understand intent, whether the customer is typing or speaking. The conversation flows are enriched and personalized by every past interaction, purchase history, and account details. When complex issues arise, these systems know how to intelligently route customers to specialists and provide them with AI-generated context, real-time insights, and response suggestions.

Business Process Automation and Decision Support
The impact of AI on core business processes reveals how this technology goes beyond simple scripted automation. Modern AI serves as a strategic partner in decision making. It’s not just handling routing tasks, it’s helping teams make smarter decisions. It’s a sidekick that can instantly process vast amounts of data, spot patterns, and suggest optimizations.

Common applications of AI in business process automation and decision support include:

Automated document processing and data extraction

Intelligent workflow routing

Fraud detection and risk assessment

Supply chain optimization

Demand forecasting

Resource allocation

Quality control and anomaly detection

Predictive maintenance

These systems can process vast amounts of data to identify patterns, make predictions, and suggest optimizations that would be impossible for humans to discover manually.

Software Development and Engineering
In software development and engineering, AI can enhance human creativity and expertise, and improve developer experience by reducing repetitive tasks, which allows developers to spend more time focused on the business logic. However it has not (yet) reached or surpassed humans, and it hasn’t eliminated the need for human oversight. Development teams now work alongside AI tools that can suggest code improvements, automate testing processes, and help maintain system health. Working with an AI coding assistant is mind-blowing. It’s like having another developer looking over your shoulder 24/7, but one that can instantly recall code patterns and best practices.

AI-powered code-generation tools and assistants such as Copilot from Github, v0 by Vercel, Bolt.new from StackBlitz, and even out-of-the-box models from OpenAI or Anthropic are invaluable to developers and can:

Generate code snippets and complete functions

Suggest optimizations and identify potential bugs

Automate testing processes

Help maintain documentation

Assist with code reviews

Generate API documentation

Create and optimize database queries

Debug complex issues

Sales and Marketing
Organizations are leveraging AI for sophisticated customer segmentation and targeting, while using content personalization to deliver tailored experiences at scale.

AI systems excel at lead scoring and qualification, helping sales teams focus their efforts on the most promising opportunities. Marketing teams benefit from AI-driven campaign optimization and comprehensive social media monitoring and analysis, while also using AI to generate marketing copy and analyze A/B tests more effectively.

The technology also helps prevent customer churn through predictive analytics and optimizes conversion rates through intelligent customer behavior analysis. Together, these AI-powered tools enable teams to create more effective campaigns, identify promising leads, and optimize their marketing spend with unprecedented precision.

Health Care
Health care providers use AI for sophisticated medical image analysis, more accurate disease diagnosis, and personalized treatment planning.

The technology enables continuous patient monitoring and automated drug interaction checking, while streamlining health care records management and appointment scheduling. AI also facilitates better matching of patients with appropriate clinical trials and enables comprehensive population health analysis.

AI allows health care providers to offer better patient care while simultaneously reducing costs and administrative burdens, ultimately leading to more efficient and effective health care systems.

The Challenges of Implementing AI and LLMs
As organizations move from experimental AI projects to production deployments, they encounter complex challenges that traditional networking infrastructure wasn’t designed to address. Understanding these challenges is crucial for building effective, secure, and governable AI systems.

Security and Access Control
The most critical security challenges that organizations face when adopting AI center around data privacy and provider trust. Organizations must consider the risk of their proprietary or sensitive information being incorporated into training datasets or being exposed through provider breaches. These concerns are particularly acute given that compromised provider systems could potentially expose not just the raw data, but also queries and responses that might reveal organizational insights or strategies.

Beyond these fundamental concerns, organizations must also tackle the practical challenges of managing access to AI providers securely and efficiently. Unlike traditional APIs, AI services require dynamic access control that can adapt to varying usage patterns and security requirements. Traditional networking tools like routers and firewalls fall short, as they often lack provisions to manage credentials or semantically understand data in transit properly.

Organizations frequently struggle with API key proliferation, where keys might accidentally be hardcoded into applications or exposed in configuration files. This challenge is compounded by inconsistent access controls across teams and applications, making it difficult to maintain security standards. Additionally, organizations must navigate complex regulatory data compliance requirements for personally identifiable information (PII), the Health Insurance Portability and Accountability Act (HIPAA), and the General Data Protection Regulation (GDPR), which traditional security tools aren’t equipped to handle due to their lack of semantic understanding.

Prompt Management and Data Protection
When employees interact with LLMs, they might include sensitive company or customer information in their prompts as the context to the LLM. Similarly, the LLMs might inadvertently include sensitive information in their responses to the users. This creates a significant security challenge: how do you prevent unauthorized data exposure while still allowing productive AI use?

Organizations need comprehensive prompt management capabilities that include robust guardrails for detecting and masking sensitive information. These systems must provide consistent prompt augmentation across all interactions while maintaining standardized policies for prompt structure and content. Similar to how web application firewalls (WAFs) protect traditional web applications, AI systems require specialized input validation and output constraints to ensure data security and compliance.

Consumption Control and Resource Optimization
LLM providers like OpenAI, Anthropic, and others make their models available through REST APIs. At a technical level, using an LLM is similar to making any other API call—you send a request with your text and receive the model’s response. This consumption-based nature of LLMs creates unique challenges for cost management and resource optimization. Organizations struggle with attributing costs across different teams and projects, especially as usage patterns vary significantly. This requires sophisticated systems for tracking and controlling resource consumption through token-weighted controls.

Semantic caching presents a particular opportunity for optimization, as many similar queries can be served from cache rather than making redundant API calls. Organizations must also implement intelligent model selection strategies, choosing the most cost-effective model for each use case while maintaining quality standards.

Resilience and Performance
Traditional load-balancing strategies fall short in AI systems because they don’t account for the semantic nature of queries or key performance metrics like latency (time to first token, or TTFT) and throughput (tokens per second, or TPS). Organizations need intelligent, context-aware systems that can effectively cache similar queries and distribute workloads based on semantic understanding rather than simple request patterns.

Failover mechanisms in AI systems must be more sophisticated than in traditional applications, as organizations often need to switch between different AI providers or models based on performance, cost, or specific use case requirements, while maintaining consistent TTFT and TPS. Cost management adds another layer of complexity, as LLMs operate on a pay-as-you-use basis, where costs are calculated based on tokens, individual units of text that LLMs process. For example, while “dog” counts as one token, longer words like “companionship” might be broken into multiple tokens. This token-based pricing model means that failover decisions must balance performance requirements with cost implications.

These challenges cannot be effectively addressed through simple application-level preprocessing or basic routing logic. Such approaches would require each application to independently implement complex semantic analysis, maintain multiple vendor connections, handle failover logic, and manage token-based cost optimization—creating substantial duplication of effort and increasing the risk of inconsistencies across the organization. Moreover, application-level solutions would lack the centralized visibility needed for effective cost management and compliance monitoring.

This is where AI gateways come in. They are specialized tools that sit between applications and AI services and provide comprehensive solutions for these challenges. An AI gateway serves as a control plane for managing AI operations, providing organization-wide semantic request routing, intelligent caching, cost optimization, and AI provider failover capabilities. By handling these concerns at the infrastructure level rather than the application level, AI gateways enable organizations to implement consistent policies and optimizations across all their AI workloads, while reducing complexity for application developers.

Barriers to Seamless AI Integration
While organizations understand AI’s transformative potential, several fundamental barriers can impede successful integration into existing enterprise environments.

Most enterprises operate complex landscapes of legacy systems that weren’t designed with AI in mind. These systems often use outdated data formats, incompatible APIs, and rigid architectures, with data siloed across disparate sources—from isolated relational databases and cloud-based document stores like SharePoint to flat files scattered across file servers and employee workstations. Some information may also exist only in physical formats like paper records or legacy offline media. This fragmented data landscape, combined with rigid architectures, makes AI integration particularly challenging. Real-time data access, essential for many AI applications, becomes especially problematic when dealing with such distributed and heterogeneous legacy infrastructure.

Beyond technical challenges, organizations face significant operational hurdles. The shift to AI-powered systems requires new skills, roles, and processes. Teams need to develop expertise in prompt engineering, model selection, and AI operations. Overcoming resistance to AI adoption often requires substantial training investment and cultural change.

AI systems are only as good as their data inputs. Organizations struggle with inconsistent data formats, outdated information across systems, and real-time synchronization challenges. Without proper data standardization and cleaning processes, AI implementations may deliver unreliable or inconsistent results.

Integrating AI capabilities across an enterprise requires careful architectural consideration. Organizations must manage multiple AI providers, handle different response formats, implement robust error handling, and ensure system reliability. This complexity increases with each new AI integration point, making centralized management and monitoring essential.

These barriers highlight the need for a solution that can abstract away complexity while providing the necessary controls and optimizations for enterprise AI deployment.

Overview of the AI Gateway Solution
An AI gateway is a specialized API gateway that can semantically understand requests and responses to handle and manage AI interactions. This understanding enables capabilities that traditional networking solutions cannot provide. It serves as an intelligent intermediary for AI traffic, providing sophisticated management capabilities that enhance applications’ interactions with AI services. This intelligence is crucial for several reasons.

First, the gateway provides sophisticated credentials and data management that traditional networking components like firewalls cannot deliver. It securely handles API keys, prevents sensitive data exposure, and ensures compliance with regulatory requirements—challenges that become increasingly complex as organizations scale their AI usage. Moreover, the AI gateway makes the processing of these concerns consistent across all infrastructure, including across multiple LLM instances.

Second, it implements AI-specific performance optimizations. The gateway can intelligently load balance requests based on their semantic content, cache similar queries to reduce costs and latency, and manage sophisticated failover between different AI providers when needed.

Third, it provides comprehensive visibility and control over LLM operations. Organizations can monitor usage patterns, implement cost controls based on token consumption, and enforce guardrails that prevent harmful or noncompliant LLM interactions. These capabilities prove essential as organizations scale from initial AI experiments to production deployments.

As we’ll examine in the following chapters, traditional networking approaches fail to handle these unique aspects of AI systems. Understanding these limitations—and how an AI gateway addresses them—is crucial for organizations looking to build robust, secure, and scalable AI infrastructure. We’ll explore the specific challenges around security and compliance, resilience, and operational control, then dive into the architectural patterns and best practices for implementing an AI gateway in your environment.

Chapter 2. The AI Gateway: Bridging the Gaps
As organizations embrace AI and LLMs, architects and IT leaders must prioritize the safe and secure adoption of these transformative technologies. In this chapter, we’ll explore the new challenges that arise when applications integrate LLMs and why traditional approaches—like conventional networking—must evolve to incorporate specialized tools tailored to this new paradigm. We will take a look at using a smart intermediary, an AI gateway, and how it improves security, observability, and performance for AI/LLM interaction, which complement traditional networking.

Why Traditional Networking Falls Short for AI Systems
To understand where traditional networking falls short, let’s take a look at a typical LLM interaction. LLMs are typically exposed through an API interface. Many LLMs have standardized on the OpenAI REST API but there are other options as well (e.g., Amazon Bedrock and Google Vertex). To make a call to an LLM, a client will prepare an HTTP request with the appropriate API keys in the headers and an appropriate prompt message for the LLM and send it over the network. For example, a client calling OpenAI from the command line would prepare a request like this:

curl https://api.openai.com/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $YOUR_API_KEY" \
  -d '{
    "model": "gpt-3.5-turbo",
    "messages": [
      {"role": "user", "content": "Hello, how are you?"}
    ]
  }'
In a typical enterprise system, this HTTP call flows through the network as shown in Figure 2-1. The packets that make up the HTTP request will flow through routers to a firewall that is responsible for enforcing policy such as “is this packet allowed to go to an IP address or network?” From there it will be routed to the internet through an L3/L4 internet gateway. The OpenAI service will receive the request, process it, and return a result, which will flow back through the enterprise network and eventually back to the client.


Figure 2-1. Requests flow from a client to the LLM through a typical enterprise network that includes firewalls and routers
You may be asking, “If this is a simple HTTP request, what’s so special about LLMs?” Great question. This simple LLM API call brings up a number of scenarios that are similar to existing HTTP requests, but also introduces some differences that can cause issues that require specialized API tooling. Let’s dig into what those issues are.

Security Practices
The first thing you may notice is that the call to the LLM requires an API key that gets sent in one of the headers. This API key is sensitive and should only be sent on HTTPS requests so it’s encrypted on the wire. This happens transparently when calling the OpenAI API for (i.e., by calling https://api.openai.com). But what about storing and retrieving the key to be used in the request? How will developers keep the key safe? If the key is compromised, whether through some nefarious act or by sharing the key with other teams, it will become a security issue.

Unfortunately, with more scale, this means many sensitive API keys scattered around a production environment and higher chances for these keys to get compromised. Not all teams will correctly follow secure practices all of the time. For example, a common scenario is for developers to accidentally hardcode API keys directly into their code and then check that code into a repository. Another practice, unfortunately common, is to see API keys specified in environment-specific configuration files. These key leaks can be very expensive.

An AI gateway can complement traditional networking security to solve these challenges. We’ll cover how to do this in Chapter 3.

Resilience
Latency and load balancing pose significant challenges when integrating LLMs into enterprise workflows. Traditional load-balancing methods, such as round-robin or connection-based strategies, are too simplistic because they don’t take into account the semantic meanings of the prompts or help decide which model is best suited to service a request. Additionally, without semantic understanding, there is no effective way to cache responses for repeated queries, which could significantly reduce latency and server load. Enterprises need intelligent, context-aware systems that can cache similar or identical semantic queries and distribute workloads more effectively to minimize response times and improve system performance.

Adding to the complexity, enterprises often need the flexibility to switch between model providers—such as OpenAI, Anthropic, or custom domain-specific models—based on strategic motivations, cost, or specialized use cases. This requires infrastructure capable of dynamically rerouting workloads while maintaining performance, minimizing downtime, and ensuring seamless transitions across models, whether they’re general purpose or tailored for specific needs.

Another concern is model failover and reliability. AI models may experience performance degradation due to model-specific issues, such as high latency during peak usage or outdated training data that fails to deliver accurate responses. Ensuring robust failover mechanisms for these models is essential. For instance, enterprises need strategies to redirect traffic seamlessly to backup models or alternate providers when the primary model isn’t performing adequately. This type of failover is more complex than standard systems, as it must account for differences in model output quality and cost while maintaining a consistent user experience.

Cost Control and Observability
Controlling costs is a critical consideration for organizations integrating LLMs into their workflows. The dynamic nature of LLM usage often makes it difficult to track who is calling these services, under which budget, and how much is being spent. Without proper visibility, enterprises risk spiraling costs as usage grows unpredictably across teams and projects. Implementing mechanisms to monitor and attribute LLM usage to specific teams or applications can provide clarity. Furthermore, setting rate limits based on token consumption (tokens are an LLM-specific concept) or enterprise-established quotas ensures that resources are used efficiently and within budget. These limits are also essential to prevent excessive usage that could lead to denial-of-service (DoS) conditions or resource starvation for critical operations.

Observability is another crucial aspect of managing LLM interactions. Enterprises must monitor key networking metrics such as error rates and latency to identify performance bottlenecks or systemic issues in their LLM workflows. However, LLMs introduce additional, model-specific concerns that go beyond traditional observability practices. For example, tracking prompt evolution (are prompts excessively long? is the structure changing to deliver sub-optimal results?), token latency (TTFT, time per output token, etc.), context-window usage, and usage attribution are all concerns.

Guardrails and Compliance
Application developers may feel free to call an LLM to implement some part of their applications, but how often do they consider the sensitivity of what is being sent in those prompts? In an enterprise setting, sending sensitive data is not just a simple oversight; it’s a major compliance risk. Enterprises operate under strict data protection and confidentiality requirements. You can’t just send data flying around without adhering to those constraints. The same is true when working with LLMs. Sensitive data—whether it’s PII, health-related under HIPAA, or subject to regional protections like GDPR—must be handled with the utmost care to meet regulatory requirements. In fact, some of the most important data, which may not be covered under compliance oversight, but is extremely sensitive, is a company’s proprietary data and trade secrets. Not all teams will have to deal with regulated data like PII, but most teams will deal with sensitive company-specific data.

This is where guardrails come into play. Think of them as the WAF for your LLM queries—enforcing rules about what’s acceptable, validating inputs, and constraining outputs. These guardrails are crucial as you scale your AI operations. For instance, they can block prompts designed to elicit biased or harmful responses, or that risk exposing sensitive information. Now, you might wonder: can’t developers just bake these protections into the app code? Sure, but that approach is risky. Just like we don’t leave the entire burden of cybersecurity to developers, the same principle applies here. A “defense-in-depth” strategy is essential. Enterprises succeed in cybersecurity by layering robust, externalized controls over application-level protections, and the same approach applies to LLMs. A well-designed governance layer that combines cost control, observability, and intelligent guardrails isn’t just a nice-to-have—it’s the key to scaling AI responsibly, predictably, and confidently.

Tackling these LLM-specific challenges like resilience, guardrails, and security will require tooling that is specialized and can take these challenges into account. Traditional networking can be augmented with specific tools like an AI gateway to solve these challenges. In the next section, we look at how to approach an AI gateway.

The Role of an AI Gateway in Secure and Scalable AI Adoption
An AI gateway is a specialized API gateway that can semantically understand the details of a request to better implement security, guardrails, observability, failover, and load balancing. You can use an AI gateway in conjunction with your existing API gateway or as a modern option to replace last-generation API management for cloud-first, platform-based, self-service deployments. An AI gateway is typically used for requests going out to an LLM that’s a publicly hosted service. As you can see in Figure 2-2, calls go from a client to an AI gateway and then out to the LLM (OpenAI, in this case). In this scenario, the AI gateway acts as a powerful forwarding proxy.

An AI gateway plays a pivotal role in helping enterprises adopt and scale AI/LLM usage securely and efficiently. Many organizations begin their AI journey by using public models like OpenAI for their flexibility and ease of integration. However, as adoption grows, they often transition to private or hybrid setups to address concerns regarding cost, data privacy, and compliance.


Figure 2-2. Requests to the LLM follow a forwarding proxy model
Security and privacy are foundational aspects of an AI gateway. Organizations must ensure that sensitive enterprise data is not inadvertently exposed to public LLMs. Customizable guardrails enable masking, blocking, or filtering of sensitive data in requests and responses, protecting data at every stage of the interaction. Additionally, centralized credential management secures API keys using secret stores and enforces fine-grained access with role-based access control (RBAC). Combined with robust authentication policies like JSON Web Tokens (JWTs) and external authentication mechanisms, the gateway ensures only authorized users can access AI APIs.

AI gateways enable operational visibility. Detailed metrics, such as request volume, token usage, and latency, provide insights for managing performance and cost. Robust logging and monitoring facilitate compliance reporting, troubleshooting, and auditing, ensuring AI adoption aligns with regulatory requirements. Integrating with existing observability tooling through open standards like OpenTelemetry helps augment existing initiatives to build smart dashboards for end users.

Since AI gateways utilize semantic understanding of requests/prompts, they can make smarter decisions about load balancing, failover, or even caching of the results. Semantic awareness enables the gateway to attach additional context to the prompt following organization-approved methods. For example, an AI gateway can help implement retrieval-augmented generation (RAG), a mechanism to add additional facts or organization-specific data to a prompt to guide the LLM to more accurate responses.

Designing and Deploying an AI Gateway: Best Practices
Many organizations are moving to highly automated, self-service workflows for their developers based around an internal developer platform, and an AI gateway must fit in with this platform. We’ll cover more about this in the last section of this chapter, but to facilitate this integration, we must take the following into account when considering AI gateway design:

Separation of concerns

Powerful foundational proxy

Declarative configuration

Open source

Separation of Concerns
Designing an AI gateway starts with clear separation of concerns. That is, the gateway should act as an operational layer, distinct from your application’s business logic. By decoupling these concerns, you can focus on enhancing the application’s functionality without constantly modifying operational code. For example, masking sensitive data, validating inputs, or enforcing API quotas should be implemented as part of the gateway, not embedded within the application itself.

Leverage a Powerful and Flexible Proxy
The choice of a proxy to serve as the foundation of your AI gateway is critical. The proxy should be performant, configurable, and mature. Envoy is known for its performance, scalability, and extensibility. Envoy’s powerful filter chain architecture enables flexibility, with standard plug-ins for features like rate limiting, external authentication (ext-auth), and general external processing (ext-proc). These capabilities enable the gateway to handle complex tasks, such as manipulating request and response bodies to mask sensitive data, without overburdening application code. With Envoy, you can rapidly develop and deploy advanced gateway features while ensuring the system remains performant and scalable.

Declarative Configuration
Declarative configuration is a best practice for managing an AI gateway, ensuring predictability and reducing the risk of human error. Using a declarative approach allows you to define the desired state of your gateway’s behavior, such as routing rules, security policies, and API quotas, in a structured format like YAML or JSON. This makes it easier to version control configurations, apply them consistently across environments, and automate updates. Additionally, declarative configurations integrate seamlessly with infrastructure as code (IaC) practices, promoting a unified and repeatable deployment process.

Roots in Open Source
Building your AI gateway on open source foundations provides several advantages. Open source technologies like Envoy come with large, active communities that contribute to ongoing innovation, security updates, and feature development. This reduces vendor lock-in and enables enterprises to stay ahead of evolving technical requirements. Leveraging open source software aligns with modern development practices, offering cost-effectiveness and flexibility in adapting the gateway to meet specific enterprise needs.

AI Success Requires Platform Engineering
IT organizations often struggle to get their multiple teams on the same page. Teams often make decisions in silos, leading to duplication of tools, integration challenges, and breakdowns in governance, which can result in compliance issues. Platform engineering plays a pivotal role in uniting diverse IT teams to streamline software delivery and operational excellence. By fostering collaboration among application developers, security specialists, networking experts, and others, platform engineering initiatives aim to reduce friction, enhance developer productivity, and enforce compliance standards.

AI gateways designed with automation-first principles, such as GitOps and IaC, can plug into these environments with minimal disruption. They enable enterprises to supercharge their platform efforts, embedding AI-powered insights and orchestration into the workflows that teams already use. For example, data scientists and developers can collaborate more effectively, using the AI gateway to enable self-service access to models, APIs, and infrastructure while maintaining robust security and governance. This approach not only accelerates AI adoption at scale but also reinforces the platform engineering mission of empowering teams with self-service tools and ensuring alignment across the organization.

As organizations introduce AI and LLM usage, an AI gateway becomes crucial. It acts as a bridge, weaving advanced semantic understanding into workflows to mitigate the risks of escalating costs, data privacy concerns, and compliance complexities—key issues in enterprise environments. An AI gateway designed with the previously discussed best practices (separation of concerns, powerful proxy, declarative configuration, and open source) is designed to integrate nicely into internal developer portals. In the next chapter, we look at an AI gateway implementation that was built with these best practices.

Chapter 3. Common AI Gateway Use Cases
In the previous chapter, we introduced the concept of an AI gateway and explained why traditional networking falls short. This chapter will explore how enterprises can use an AI gateway to solve real-world challenges.

As we explore these use cases, we’ll reference an AI gateway implementation as an example. Users should choose an AI gateway built on modern architectures including Envoy Proxy and Kubernetes Gateway API. Gateways like Gloo AI Gateway (being donated to Cloud Native Computing Foundation—CNCF—as KGateway) enable AI features that accelerate AI application development while addressing critical security, observability, control, and governance needs.

Choosing a reliable AI gateway helps address the top concerns that enterprises have when adopting AI, including:

Security threat mitigation

The bridging of the technical skills gap

Seamless integration with existing infrastructure

With this context in mind, let’s explore the key use cases that make AI gateways essential for enterprise AI adoption.

Security and Access Control
Security and access control are paramount for every technology, and AI use cases aren’t any different. An AI gateway can encompass everything from managing API credentials to multi-tenant isolation.

In this section, we’ll explore how an AI gateway provides a security layer that handles authentication, authorization, and access policies to ensure that AI resources and LLM providers are accessed only by authorized users and applications.

API Key and Credential Management
Because every LLM provider requires an API key, managing them presents a significant security challenge in enterprise environments. The traditional approach of embedding API keys directly in application code or configuration files leads to credential sprawl, increasing the risk of accidental exposure through code repositories or configuration leaks. As organizations scale their AI adoption and integrate multiple LLM providers, this problem is compounded.

Figure 3-1 shows how an AI gateway addresses these challenges by centralizing credential management at the infrastructure level, away from the application code. This allows operators to securely store and manage API keys for multiple providers while integrating with enterprise secret management systems, as shown in Figure 3-2.


Figure 3-1. Comparison of direct versus gateway-mediated access to LLMs

Figure 3-2. Integration of enterprise secrets management system with an AI gateway architecture
By removing credential management responsibilities from application teams, organizations can implement consistent security practices, automate key rotation, and maintain a comprehensive audit trail of credential usage across all AI workloads.

Fine-Grained Access Control and Auditing
While centralizing LLM credential management solves one part of the security equation, organizations also need to control access to LLMs. Users should select an AI gateway that can integrate with existing enterprise authentication and authorization mechanisms. Whether an organization uses JWTs, custom tokens, OpenID Connect (OIDC), Lightweight Directory Access Protocol (LDAP), or other authentication methods, AI gateways can leverage these existing systems rather than introducing new credentials to manage.

Let’s look at an example using JWTs, though similar principles apply to other authentication methods. JWTs serve as one approach for implementing fine-grained access control in AI gateways.

Unlike API keys, JWTs carry rich claims (key-value pairs) about the user’s identity, roles, permissions, and any other values. This enables fine-grained and flexible access control decisions.

Here’s an example of a JWT with custom claims:

{
  "iss": "sso.solo.io",
  "sub": "peterj",
  "team": "ai-research",
  "llms": {
    "openai": ["gpt-4", "gpt-3.5-turbo"],
    "anthropic": ["claude-3-opus"]
  }
}
Based on the claims in the JWT, the AI gateway can enforce access control at multiple levels. Let’s look at a couple of examples.

Team and LLM access control
This example restricts which models can be accessed or enforces team isolation (that is, restricts access by team) based on the values in the JWT claims using RBAC, as shown in Figure 3-3. The example only allows access to the openai-route if JWT contains the team: ai-research value:

apiVersion: gateway.solo.io/v1
kind: RouteOption
metadata:
  name: jwt-ai-research
spec:
  targetRefs:
  - group: gateway.networking.k8s.io
    kind: HTTPRoute
    name: openai-route
  options:
    rbac:
      policies:
        viewer:
          nestedClaimDelimiter: .
          principals:
          - jwtPrincipal:
              claims:
                team: ai-research

Figure 3-3. Using RBAC to allow access to the AI research team and restricting access for the QA team through an AI gateway
Similarly, we could come up with a configuration that restricts which models a user can access based on the JWT claims.

Usage control
The gateway operators can use a declarative configuration to enforce token rate limits based on claims in the JWT as shown in Figure 3-4. The example below configures a rate limit of 500 tokens per hour for each JWT subject (sub) claim. This rate limit configuration is also separately applied to a specific route on which the AI gateway enforces it:

...
    descriptors:
    - key: user-id
      rateLimit:
        requestsPerUnit: 500
        unit: HOUR
    rateLimits:
    - actions:
      - metadata:
          descriptorKey: user-id
          source: DYNAMIC
          default: unknown
          metadataKey:
            key: "envoy.filters.http.jwt_authn"
            path:
            - key: principal
            - key: sub

Figure 3-4. Token rate limiting with per-subject quota from JWT
Securing Multi-Tenancy in AI Workflows
Large enterprises typically have multiple business units, development teams, or external partners that need access to AI capabilities. Consider a financial services company where different departments—wealth management, retail banking, insurance—all need to use LLMs for various purposes. Each department has specific security requirements, data handling needs, and budget constraints.

By combining an AI gateway and a solution such as a service mesh, enterprise teams can enable secure multi-tenancy. This allows organizations to isolate each tenant while maintaining control and governance.

For example, the wealth management division might need to use OpenAI’s GPT-4o model for analyzing investment reports, while the retail banking team uses a different model or provider for customer service automation.

With this approach, you can ensure the following for each tenant:

Separate API credentials and rate limits

Division-specific prompt templates and configurations

Isolated data handling policies

Individual cost centers and usage quotas

Custom security policies and compliance rules

Data Privacy and Compliance Enforcement (Guardrails)
Modern enterprises navigate large sets of private and sensitive data on a day-to-day basis, requiring strict data compliance and confidentiality provisions. Additionally, all sensitive data in enterprises must follow regulatory and compliance requirements, including HIPAA, GDPR, and other applicable regional requirements such as the California Consumer Privacy Act (CCPA).

An AI gateway serves as an intermediary that inspects, filters, and governs all data flowing between all applications and AI models and backends. In this section, we’ll explore how enterprises can implement data protection strategies, including content filtering and PII detection.

To protect data, enterprises must implement guardrails and data loss prevention (DLP) strategies. The guardrails in the AI gateway provide bidirectional protection for both incoming requests (prompts or queries from the end users or applications) and outgoing responses from the LLMs.

The protection can be done by simply rejecting responses and returning a predefined message (such as, “Rejected due to inappropriate content”) or by masking them. Masking involves replacing the sensitive content with a specific word or character. For example, if we want to mask locations, the string “Italy is a great place to visit” might look like this: “[LOCATION] is a great place to visit.” Another option is to completely mask the detected string without giving a clue as to what it might be: “XXXXX is a great place to visit.”

For incoming requests on the gateway, the guardrails kick in before the request is sent to the LLM. This allows detection and automatic blocking of requests before they reach the LLM, ensuring compliance with data privacy regulations. This approach prevents any sensitive data from being inadvertently shared with external AI services.

On the response side, the LLM outputs may be enriched with enterprise data that contains sensitive information. While useful as a context and to provide the most accurate information, it’s not necessarily desirable to return the sensitive data to the caller. The guardrails on the response can detect PII and apply masking rules or, when necessary, reject the response entirely and replace it with a predefined message such as “Rejected.” This ensures that protected information remains secure even when LLMs incorporate internal data sources. Figure 3-5 shows the flow of unsanitized and sanitized inputs and outputs with input and output guardrails.


Figure 3-5. Content sanitization workflow with input and output guardrails within an AI gateway
A guardrails system—for example, Microsoft Presidio—typically employs one or more of the following methods to evaluate and filter content in both directions:

Built-in patterns
Can be used to detect credit card numbers, social security numbers, emails, and phone numbers.

Regular expressions
Can be used to reject or mask the matched strings.

External moderation
Can be used to determine whether the request/response should be masked or rejected. This may be an external model such as omni-moderation from OpenAI or a custom webhook.

Beyond direct content filtering, organizations can further enhance privacy and compliance through prompt templates. These templates allow system prompts or additional context to be automatically attached to user queries, establishing consistent ground rules for all LLM interactions.

For example, in Figure 3-6 we automatically attach a system prompt instructing the LLM to perform sentiment analysis on the user’s query and respond with a single word in French. With the ability to do this through the AI gateway, organizations can define and enforce standardized system prompts and automatically apply them to all client requests, ensuring adherence to privacy policies and compliance requirements without requiring individual applications to specify these rules in each query. This templating approach creates a unified governance layer that standardizes how applications interact with LLMs while maintaining security protocols.


Figure 3-6. Composing user input and predefined system template allowing standardized LLM interaction
Performance and Reliability
AI workloads present unique challenges in terms of performance and reliability management. From handling varying response times to managing high-concurrency scenarios, AI gateways must ensure consistent and reliable service delivery.

Performance Metrics and User Experience
When managing AI workloads through an AI gateway, understanding and optimizing key performance metrics is crucial for service reliability. The primary metrics that impact user experience are latency-based, particularly in streaming scenarios where responses are generated token by token. By default, the responses from the LLM providers are returned once the complete response is generated.

Two critical metrics for monitoring gateway performance are TTFT (which we discussed in Chapter 1) and time per output token (TPOT).

TTFT measures the initial response time from query submission to the first generated token. This metric is important for real-time applications like chatbots, where users expect immediate responses. The AI gateway’s traffic management decisions can significantly impact TTFT by optimizing routing and load distribution.

TPOT measures the generation speed of subsequent tokens. For streaming responses, TPOT should align with human reading speed (approximately 5-6 tokens per second, depending on the complexity of the text and reading speed) to ensure a smooth user experience. The AI gateway can optimize TPOT through intelligent load balancing and resource allocation.

To calculate the total latency, take the TTFT and add it to the product of the TPOT and the number of output tokens, as shown in Figure 3-7.

When estimating expected latency, be mindful that response times can be delayed if the AI agent you’re interacting with is under a heavy workload and performing multiple LLM requests as part of the same interaction. For that reason, it makes sense for your performance tests to generate a workload heavy enough to reflect expected production conditions.


Figure 3-7. Latency components in LLM response generation
Traffic Management and Scaling for AI Workloads
Robust traffic management is crucial for maintaining reliable and cost-effective services. An AI gateway needs to efficiently route requests, balance loads across endpoints, and enforce policies while ensuring optimal performance and resource utilization.

For example, Gloo AI Gateway—also known as KGateway—leverages Envoy proxy’s traffic management capabilities to handle the AI workloads running across multiple teams and accessing different providers.

In this scenario, Gloo AI Gateway implements both distributed and global load-balancing strategies to optimize traffic flow. It uses active health checking and intelligent load-balancing algorithms to route requests across model endpoints based on their current performance, availability, and API quota. At the global level, the gateway will enforce organization-wide traffic policies, such as preferring certain model providers or endpoints based on cost, performance, or geographical location.

The routing and traffic management are based on various request attributes. For example, organizations can configure routing rules that direct requests to different model endpoints based on:

Request headers that indicate the complexity or priority of the query

Path parameters that specify model capabilities or versions

Team or application identifiers that determine routing preferences

Resource utilization patterns that optimize for cost and performance

Provider token usage limits (i.e., route to different models when token limits are reached)

Health checking and outlier detection mechanisms continuously monitor endpoint health, automatically removing degraded instances from the load-balancing pool. In case of failures, the AI gateway can automatically failover and switch to a backup system.

In this example, Gloo AI Gateway can also handle failover for the models of the LLM providers that you want to prioritize. If the main model from one provider is unavailable, becomes slow, or has any issues, the AI gateway can quickly switch to a backup model from the same or different provider. This keeps your services running without interruption.

Integration of Hybrid AI Solutions
While public, cloud-based LLMs are probably the easiest way to start working on a proof of concept, organizations often need to leverage multiple models, both public and private, deployed across different infrastructures. This hybrid approach enables enterprises to balance the advanced capabilities of various public models with the security and control offered by different private model deployments. An AI gateway is in the perfect position to serve as an orchestration layer for managing this architecture and providing unified routing, security, and governance across all types of models.

For example, an enterprise might use a public model like GPT-4o for general-purpose tasks, while routing sensitive queries involving private data to custom or fine-tuned models running on its own infrastructure. The AI gateway can make these routing decisions based on factors including the content type, security requirements, or specific headers in the request. This intelligent routing ensures that each query is directed to the most appropriate model while maintaining consistent security policies and monitoring across the entire infrastructure and AI resources.

Optimization and Monitoring
As AI usage grows within an organization, understanding usage patterns, controlling costs, and maintaining visibility into system performance becomes increasingly important.

In this section, we’ll explore how AI gateways can help enterprises optimize their AI investments through features like semantic caching and intelligent routing, while providing the observability needed to maintain and improve system performance.

Cost Optimization and Budget Controls
Semantic caching is an approach to reducing computational costs and improving response times when working with LLMs. Unlike traditional caching that relies on exact matches, semantic caching understands the intent behind queries, allowing for the reuse of previous responses even when questions are phrased differently.

Since every response generated by an LLM incurs some cost, every positive hit in the semantic cache means fewer cost-incurring requests made to the LLM provider. Semantic caching also improves response times when compared to responses generated by an LLM, typically measured in seconds. Better response times mean better user experience.

Let’s take a look at an example. Consider a customer service AI that handles product inquiries. If one customer asks “What’s the return policy for electronics?” and another later asks “How many days do I have to return my laptop?”, traditional caching would treat these as different queries requiring separate calls to the LLM. However, semantic caching recognizes their similar intent and can serve the cached response from the first query to the second customer, saving both time and cost, as shown in Figure 3-8.


Figure 3-8. Using a vector database to retrieve semantically similar requests
It’s important to note that implementing semantic caching itself requires embedding models or smaller LLMs at the gateway level to analyze query intent, which introduces its own computational overhead and complexity. This means organizations need to carefully evaluate the trade-off between gateway costs and potential savings from reduced LLM calls.

The cost savings of semantic caching can be substantial. If we continue with the previous example, let’s say there are 100,000 customer queries per month using GPT-4, with an average cost of $0.03 per query. Without semantic caching, this would cost $3,000 a month. If semantic caching can handle 60% of queries (a conservative estimate for common customer service scenarios), the monthly cost drops to $1,200—a savings of $1,800. Over the course of a year, this represents $21,600 in direct LLM cost reductions, not including the additional savings from reduced computational resources and improved response times.

Another optimization that impacts latency, and improves perceived responsiveness and TTFT metrics is streaming mode. In streaming mode, the LLM sends tokens to the client as they are generated, rather than waiting for the entire response to be completed, allowing users to see the response build progressively in real time. Streaming mode is provided by the LLM backend and must be implemented and enabled in the AI gateway.

Monitoring and Observability for AI Pipelines
Just like for any application, good monitoring and observability are crucial when running applications that use AI and LLMs. The operating unit of measurement and cost is tokens. They are the units that organizations are charged for. Both input and output tokens matter: the input tokens are the prompts and queries that are sent to the LLM and the output tokens represent the response from the LLM. The costs for both are typically represented per one million tokens and differ by LLM provider.

Depending on the AI provider and the model you’re using, you should expect the cost to be anywhere from a few cents to a few dollars per million input tokens, while the output token responses will typically be a few times more expensive.

With varying costs for tokens, it’s clear why comprehensive monitoring is important. An efficient observability system enables organizations to identify issues early, allowing them to optimize model usage, track costs, and ensure that budgets are not exceeded.

In “Performance Metrics and User Experience”, we talked about the performance metrics TTFT and TPOT. Here are some other metrics, grouped into categories, which can provide valuable insights into your AI applications:

Resource utilization

Token consumption patterns by model and endpoint

Request distribution across different LLM providers

Semantic cache hit rates and efficiency metrics

Concurrent request patterns and queue depths

Quality metrics

Completion success rates

Error rates and types (context length, content filtering, timeout)

Response coherence scores

Hallucination detection rates

Financial metrics

Cost per request by model

Token utilization efficiency

Budget adherence by team/project

Cost anomaly detection

User experience

TTFT

TPOT

TPS

Total request latency distribution

Conclusion
AI gateways serve as a critical foundation for enterprise AI adoption by providing comprehensive solutions for security, performance optimization, and observability challenges. Features like centralized credential management, semantic caching, and detailed monitoring capabilities enable organizations to safely scale their AI implementations while maintaining control over costs, security, and compliance requirements.

In the next chapter, we’ll look ahead at how these foundational capabilities will evolve in the future.

Chapter 4. The Future of Enterprise AI
AI makes for an exciting and innovative period of time in information technology. While many organizations are leveraging AI to either optimize and make business workflows more efficient, or to innovate and provide better user experiences, we believe more and more use cases will take advantage of this technology. Scaling AI use cases and adoption will require specialized tooling that is “AI-aware” such as an AI gateway as discussed in this report. Let’s see how the future of AI in enterprises contributes to the challenges of scale, security, observability, and compliance.

Improvement of Models, Continued Adoption
We expect that enterprises will continue to innovate and invest in AI and LLM-powered use cases. LLMs are helping to automate routine tasks, such as system monitoring, incident response, and infrastructure optimization, enabling faster resolution times and improved efficiency. For security and threat detection, these models enhance the ability to identify patterns, detect anomalies, and respond to cyber threats in real time. Customer service and support are also undergoing a shift, with AI-powered chatbots and virtual assistants delivering personalized, around-the-clock support that improves customer satisfaction and reduces operational costs. These investments are driving strategic business outcomes and helping businesses maintain a competitive advantage.

Alongside the continued expansion of AI-backed use cases, we expect the underlying models to become more powerful and cheaper. Over the next year, we can expect significant advancements in LLMs across several key areas. Context windows are expanding dramatically, with some models like Anthropic’s Claude already reaching 100,000 tokens, and Google’s Gemini supporting up to two million tokens. These numbers may be outdated by the time you read this, but the fact remains that these models will continue to extend context capabilities, which will allow models to analyze even larger amounts of content and datasets in a single interaction. Reasoning capabilities are also improving rapidly, as seen with OpenAI’s o1 and o3 models, which emphasize enhanced logical consistency, multistep problem solving, and contextual understanding. Lastly, the cost to train these models will likely drop as seen with the claims of the recent release of DeepSeek-R1. This progression will make AI more adept at addressing complex, nuanced queries across enterprise domains.

Adopting Internal Models
Enterprises are increasingly exploring the potential of running and training their own language models, leveraging small language models (SLMs) or open source options like Llama, Mistral, and others. This trend is driven by the need for greater control over data privacy, cost efficiency, and customization. SLMs and open source models enable organizations to tailor models to their specific domain requirements while reducing dependency on third-party vendors. These smaller, fine-tuned models can deliver competitive performance for many use cases, such as document summarization, customer support, and predictive analytics, without requiring the massive infrastructure and resources of larger LLMs.

Kubernetes plays a critical role in operationalizing these AI workloads, as it is already the strategic runtime container platform for most enterprises. Kubernetes provides the scalability, resource management, and orchestration necessary for running inference workloads efficiently across distributed environments. By containerizing model training and inference tasks, organizations can integrate AI into their existing continuous integration, continuous delivery (CI/CD) pipelines and manage workloads at scale. Moreover, tools like KServe enable the automation of model training, deployment, and monitoring, allowing teams to adopt AI in a cost-effective and production-ready manner. This synergy between Kubernetes and enterprise AI adoption aligns nicely with platform engineering efforts currently underway at most organizations to provide a consistent user experience, self-service, and improved compliance.

Better Control over Model Inference with Extensions to Gateway API
The Kubernetes community is actively advancing its support for inference workloads through initiatives like the LLM Instance Gateway proposal, developed by the WG Serving working group and sponsored by SIG Apps. Inference workloads in this context means running models that have already been trained on Kubernetes, usually utilizing some specialized hardware like GPUs. This proposal introduces specialized load-balancing algorithms, custom resource definitions (CRDs), and controllers tailored to calling LLM services, enabling efficient routing and resource management for requests going to running models, while taking into account specialized accelerators and fine-tuned models. By multiplexing LLM services over shared compute pools, the initiative aims to optimize resource utilization and improve inference performance, addressing the unique challenges of deploying AI workloads in Kubernetes environments. These efforts align with Kubernetes’ role as the strategic container runtime for modern applications, further streamlining the integration of cutting-edge AI capabilities into enterprise infrastructure while maintaining scalability, efficiency, and ease of operation.

Autonomy and Agentic Workflows
The future of AI is focused on transforming LLMs from reactive chatbots into proactive, autonomous agents capable of sophisticated multistep planning and execution. Agentic AI systems—those that can act independently and make decisions on behalf of users—represent a significant evolution beyond current capabilities. Unlike traditional LLMs, which respond passively to user prompts, these next-generation AI agents will operate with a degree of autonomy, dynamically analyzing goals, prioritizing tasks, and executing them across various domains without constant human oversight. For example, instead of simply answering a query about generating a report, an AI agent could autonomously collect relevant data, synthesize insights, format the document, and even schedule a presentation for stakeholders.

These advancements hinge on integrating LLMs with reinforcement learning, planning algorithms, and external systems. By leveraging tools and APIs, autonomous AI agents will interact seamlessly with other software, databases, and Internet of Things (IoT) devices, enabling them to perform actions like automating business workflows, managing cloud infrastructure, or even running complex simulations. Frameworks such as CrewAI or LangChain exemplify early steps in this direction, allowing AI agents to decompose high-level goals into actionable subtasks and execute them iteratively. As these agents become more sophisticated, they could revolutionize industries ranging from customer service to robotics, ushering in a new era of proactive, goal-driven AI systems that extend far beyond traditional conversational capabilities.

Next Steps
As enterprises continue to integrate AI into their operations, the complexities of managing AI workloads—particularly in areas like security, observability, compliance, and scalability—are becoming increasingly evident. To address these challenges, tools like Gloo AI Gateway (KGateway) and Portkey AI Gateway have emerged as essential components in the enterprise AI ecosystem. By implementing an AI gateway, enterprises can establish a robust framework that not only accelerates AI innovation, but also ensures that AI applications are deployed securely, efficiently, and at scale. This positions organizations to effectively harness the transformative potential of AI while maintaining control over operational complexities.

About the Authors
Christian Posta (@christianposta) is VP, Global Field CTO at Solo.io. He is the author of Istio in Action (Manning), as well as many other books on cloud native architecture, and is well known in the cloud native community for being a speaker, blogger, and contributor to various open source projects in the service mesh and cloud native ecosystem (Istio, Kubernetes, et al.). Christian has spent time at government and commercial enterprises, as well as web-scale companies, and now helps organizations create and deploy large-scale, cloud native, resilient, distributed architectures. He enjoys mentoring, training, and leading teams to be successful with distributed systems concepts, microservices, DevOps, and cloud native application design.

Peter Jausovec is a senior principal technical marketing manager at Solo.io with over 17 years of experience spanning software development, QA, and engineering leadership. A recognized expert in cloud native technologies, he specializes in Kubernetes and Istio. Peter authored the book Cloud Native (O’Reilly) and pioneered several industry-leading initiatives, including developing comprehensive Envoy proxy and Istio training courses that have educated over 15,000 learners. He created the first-ever Istio certification program, now the Istio Certified Associate exam under the Linux Foundation. At Solo.io, Peter has been instrumental in shaping next-generation technologies, including prototyping the AI Gateway.